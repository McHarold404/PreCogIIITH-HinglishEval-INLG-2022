{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3c981409",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T15:02:56.655375Z",
     "iopub.status.busy": "2022-05-08T15:02:56.654993Z",
     "iopub.status.idle": "2022-05-08T15:03:06.357673Z",
     "shell.execute_reply": "2022-05-08T15:03:06.357090Z",
     "shell.execute_reply.started": "2022-05-08T15:02:56.655294Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7da0288b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T15:03:11.908522Z",
     "iopub.status.busy": "2022-05-08T15:03:11.908298Z",
     "iopub.status.idle": "2022-05-08T15:03:12.590417Z",
     "shell.execute_reply": "2022-05-08T15:03:12.589692Z",
     "shell.execute_reply.started": "2022-05-08T15:03:11.908499Z"
    }
   },
   "outputs": [],
   "source": [
    "tdf = pd.read_json(\"../dataset/traindf.json\")\n",
    "vdf = pd.read_json(\"../dataset/valdf.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1a52e2cf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T15:03:14.644460Z",
     "iopub.status.busy": "2022-05-08T15:03:14.644237Z",
     "iopub.status.idle": "2022-05-08T15:03:14.648679Z",
     "shell.execute_reply": "2022-05-08T15:03:14.648186Z",
     "shell.execute_reply.started": "2022-05-08T15:03:14.644437Z"
    }
   },
   "outputs": [],
   "source": [
    "def returnSynthSentences(df):\n",
    "    csnliHinglish = list(df[\"Hinglish_csnliop\"])\n",
    "    sentences = []\n",
    "    for line in csnliHinglish:\n",
    "        sent = []\n",
    "        for token in line:\n",
    "            sent.append(token[1])\n",
    "        sent = \" \".join(sent)\n",
    "        sentences.append(sent)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7a22745",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T15:03:15.497569Z",
     "iopub.status.busy": "2022-05-08T15:03:15.497388Z",
     "iopub.status.idle": "2022-05-08T15:03:15.501617Z",
     "shell.execute_reply": "2022-05-08T15:03:15.501027Z",
     "shell.execute_reply.started": "2022-05-08T15:03:15.497548Z"
    }
   },
   "outputs": [],
   "source": [
    "def returnHumSentences(df):\n",
    "    csnliHinglish = list(df[\"hum_gen_csnliop\"])\n",
    "    sentences = []\n",
    "    for sents in csnliHinglish:\n",
    "        ref = []\n",
    "        for line in sents:\n",
    "            sent = []\n",
    "            for token in line:\n",
    "                sent.append(token[1])\n",
    "            sent = \" \".join(sent)\n",
    "            ref.append(sent)\n",
    "            \n",
    "        sentences.append(ref)\n",
    "    \n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d12a7225",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T15:03:17.318671Z",
     "iopub.status.busy": "2022-05-08T15:03:17.318448Z",
     "iopub.status.idle": "2022-05-08T15:03:17.356940Z",
     "shell.execute_reply": "2022-05-08T15:03:17.356318Z",
     "shell.execute_reply.started": "2022-05-08T15:03:17.318649Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2766 2766 395 395\n"
     ]
    }
   ],
   "source": [
    "trainSynth = returnSynthSentences(tdf)\n",
    "validSynth = returnSynthSentences(vdf)\n",
    "\n",
    "trainHuman = returnHumSentences(tdf)\n",
    "validHuman = returnHumSentences(vdf)\n",
    "\n",
    "print(len(trainSynth), len(trainHuman), len(validSynth), len(validHuman))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e6a9c981",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-05-08T15:03:20.849747Z",
     "iopub.status.busy": "2022-05-08T15:03:20.849544Z",
     "iopub.status.idle": "2022-05-08T15:07:10.032295Z",
     "shell.execute_reply": "2022-05-08T15:07:10.031702Z",
     "shell.execute_reply.started": "2022-05-08T15:03:20.849725Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/fairseq/archive/main.zip\" to /home/prashantk/.cache/torch/hub/main.zip\n",
      "fatal: Not a git repository (or any parent up to mount point /home)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running build_ext\n",
      "cythoning fairseq/data/data_utils_fast.pyx to fairseq/data/data_utils_fast.cpp\n",
      "cythoning fairseq/data/token_block_utils_fast.pyx to fairseq/data/token_block_utils_fast.cpp\n",
      "building 'fairseq.libbleu' extension\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbleu\n",
      "Emitting ninja build file /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/build.ninja...\n",
      "Compiling objects...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/2] c++ -MMD -MF /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/module.o.d -pthread -B /home/prashantk/miniconda3/envs/cm/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/prashantk/miniconda3/envs/cm/include/python3.7m -c -c /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/clib/libbleu/module.cpp -o /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/module.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "[2/2] c++ -MMD -MF /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/libbleu.o.d -pthread -B /home/prashantk/miniconda3/envs/cm/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/prashantk/miniconda3/envs/cm/include/python3.7m -c -c /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/clib/libbleu/libbleu.cpp -o /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/libbleu.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "creating build/lib.linux-x86_64-3.7\n",
      "creating build/lib.linux-x86_64-3.7/fairseq\n",
      "g++ -pthread -shared -B /home/prashantk/miniconda3/envs/cm/compiler_compat -L/home/prashantk/miniconda3/envs/cm/lib -Wl,-rpath=/home/prashantk/miniconda3/envs/cm/lib -Wl,--no-as-needed -Wl,--sysroot=/ /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/libbleu.o /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/module.o -o build/lib.linux-x86_64-3.7/fairseq/libbleu.cpython-37m-x86_64-linux-gnu.so\n",
      "building 'fairseq.data.data_utils_fast' extension\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/data\n",
      "Emitting ninja build file /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/build.ninja...\n",
      "Compiling objects...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/1] c++ -MMD -MF /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/data/data_utils_fast.o.d -pthread -B /home/prashantk/miniconda3/envs/cm/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include -I/home/prashantk/miniconda3/envs/cm/include/python3.7m -c -c /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/data_utils_fast.cpp -o /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/data/data_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=data_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "In file included from /home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969:0,\n",
      "                 from /home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,\n",
      "                 from /home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4,\n",
      "                 from /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/data_utils_fast.cpp:695:\n",
      "/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n",
      " #warning \"Using deprecated NumPy API, disable it with \" \\\n",
      "  ^\n",
      "creating build/lib.linux-x86_64-3.7/fairseq/data\n",
      "g++ -pthread -shared -B /home/prashantk/miniconda3/envs/cm/compiler_compat -L/home/prashantk/miniconda3/envs/cm/lib -Wl,-rpath=/home/prashantk/miniconda3/envs/cm/lib -Wl,--no-as-needed -Wl,--sysroot=/ /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/data/data_utils_fast.o -o build/lib.linux-x86_64-3.7/fairseq/data/data_utils_fast.cpython-37m-x86_64-linux-gnu.so\n",
      "building 'fairseq.data.token_block_utils_fast' extension\n",
      "Emitting ninja build file /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/build.ninja...\n",
      "Compiling objects...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/1] c++ -MMD -MF /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.o.d -pthread -B /home/prashantk/miniconda3/envs/cm/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include -I/home/prashantk/miniconda3/envs/cm/include/python3.7m -c -c /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/token_block_utils_fast.cpp -o /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=token_block_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "In file included from /home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969:0,\n",
      "                 from /home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,\n",
      "                 from /home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4,\n",
      "                 from /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/token_block_utils_fast.cpp:696:\n",
      "/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n",
      " #warning \"Using deprecated NumPy API, disable it with \" \\\n",
      "  ^\n",
      "/home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/token_block_utils_fast.cpp: In function ‘PyArrayObject* __pyx_f_7fairseq_4data_22token_block_utils_fast__get_slice_indices_fast(PyArrayObject*, PyObject*, int, int, int)’:\n",
      "/home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/token_block_utils_fast.cpp:3382:36: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "       __pyx_t_4 = ((__pyx_v_sz_idx < __pyx_t_10) != 0);\n",
      "                                    ^\n",
      "/home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/token_block_utils_fast.cpp:3577:36: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "       __pyx_t_3 = ((__pyx_v_sz_idx < __pyx_t_10) != 0);\n",
      "                                    ^\n",
      "g++ -pthread -shared -B /home/prashantk/miniconda3/envs/cm/compiler_compat -L/home/prashantk/miniconda3/envs/cm/lib -Wl,-rpath=/home/prashantk/miniconda3/envs/cm/lib -Wl,--no-as-needed -Wl,--sysroot=/ /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.o -o build/lib.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.cpython-37m-x86_64-linux-gnu.so\n",
      "building 'fairseq.libbase' extension\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbase\n",
      "Emitting ninja build file /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/build.ninja...\n",
      "Compiling objects...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/1] c++ -MMD -MF /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbase/balanced_assignment.o.d -pthread -B /home/prashantk/miniconda3/envs/cm/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/TH -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/THC -I/home/prashantk/miniconda3/envs/cm/include/python3.7m -c -c /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/clib/libbase/balanced_assignment.cpp -o /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbase/balanced_assignment.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=libbase -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "g++ -pthread -shared -B /home/prashantk/miniconda3/envs/cm/compiler_compat -L/home/prashantk/miniconda3/envs/cm/lib -Wl,-rpath=/home/prashantk/miniconda3/envs/cm/lib -Wl,--no-as-needed -Wl,--sysroot=/ /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbase/balanced_assignment.o -L/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/fairseq/libbase.cpython-37m-x86_64-linux-gnu.so\n",
      "building 'fairseq.libnat' extension\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libnat\n",
      "Emitting ninja build file /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/build.ninja...\n",
      "Compiling objects...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/1] c++ -MMD -MF /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libnat/edit_dist.o.d -pthread -B /home/prashantk/miniconda3/envs/cm/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/TH -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/THC -I/home/prashantk/miniconda3/envs/cm/include/python3.7m -c -c /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/clib/libnat/edit_dist.cpp -o /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libnat/edit_dist.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=libnat -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "g++ -pthread -shared -B /home/prashantk/miniconda3/envs/cm/compiler_compat -L/home/prashantk/miniconda3/envs/cm/lib -Wl,-rpath=/home/prashantk/miniconda3/envs/cm/lib -Wl,--no-as-needed -Wl,--sysroot=/ /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libnat/edit_dist.o -L/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/fairseq/libnat.cpython-37m-x86_64-linux-gnu.so\n",
      "building 'alignment_train_cpu_binding' extension\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/examples\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/examples/operators\n",
      "Emitting ninja build file /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/build.ninja...\n",
      "Compiling objects...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "[1/1] c++ -MMD -MF /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/examples/operators/alignment_train_cpu.o.d -pthread -B /home/prashantk/miniconda3/envs/cm/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/TH -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/THC -I/home/prashantk/miniconda3/envs/cm/include/python3.7m -c -c /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/examples/operators/alignment_train_cpu.cpp -o /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/examples/operators/alignment_train_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=alignment_train_cpu_binding -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "g++ -pthread -shared -B /home/prashantk/miniconda3/envs/cm/compiler_compat -L/home/prashantk/miniconda3/envs/cm/lib -Wl,-rpath=/home/prashantk/miniconda3/envs/cm/lib -Wl,--no-as-needed -Wl,--sysroot=/ /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/examples/operators/alignment_train_cpu.o -L/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/alignment_train_cpu_binding.cpython-37m-x86_64-linux-gnu.so\n",
      "copying build/lib.linux-x86_64-3.7/fairseq/libbleu.cpython-37m-x86_64-linux-gnu.so -> fairseq\n",
      "copying build/lib.linux-x86_64-3.7/fairseq/data/data_utils_fast.cpython-37m-x86_64-linux-gnu.so -> fairseq/data\n",
      "copying build/lib.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.cpython-37m-x86_64-linux-gnu.so -> fairseq/data\n",
      "copying build/lib.linux-x86_64-3.7/fairseq/libbase.cpython-37m-x86_64-linux-gnu.so -> fairseq\n",
      "copying build/lib.linux-x86_64-3.7/fairseq/libnat.cpython-37m-x86_64-linux-gnu.so -> fairseq\n",
      "copying build/lib.linux-x86_64-3.7/alignment_train_cpu_binding.cpython-37m-x86_64-linux-gnu.so -> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 20:34:47 | INFO | fairseq.file_utils | http://dl.fbaipublicfiles.com/fairseq/models/xlmr.large.tar.gz not found in cache, downloading to /tmp/tmpqd25_qcm\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1028340964/1028340964 [01:08<00:00, 14918542.38B/s]\n",
      "2022-05-08 20:35:56 | INFO | fairseq.file_utils | copying /tmp/tmpqd25_qcm to cache at /home/prashantk/.cache/torch/pytorch_fairseq/3f864e15bb396f062dd37494309dbc4238416edd1f8ef293df18b1424813f2fe.cf46c7deb6b9eaa3e47c17b9fc181669c52bc639c165fbc69166a61487662ac9\n",
      "2022-05-08 20:36:06 | INFO | fairseq.file_utils | creating metadata file for /home/prashantk/.cache/torch/pytorch_fairseq/3f864e15bb396f062dd37494309dbc4238416edd1f8ef293df18b1424813f2fe.cf46c7deb6b9eaa3e47c17b9fc181669c52bc639c165fbc69166a61487662ac9\n",
      "2022-05-08 20:36:06 | INFO | fairseq.file_utils | removing temp file /tmp/tmpqd25_qcm\n",
      "2022-05-08 20:36:06 | INFO | fairseq.file_utils | loading archive file http://dl.fbaipublicfiles.com/fairseq/models/xlmr.large.tar.gz from cache at /home/prashantk/.cache/torch/pytorch_fairseq/3f864e15bb396f062dd37494309dbc4238416edd1f8ef293df18b1424813f2fe.cf46c7deb6b9eaa3e47c17b9fc181669c52bc639c165fbc69166a61487662ac9\n",
      "2022-05-08 20:36:06 | INFO | fairseq.file_utils | extracting archive file /home/prashantk/.cache/torch/pytorch_fairseq/3f864e15bb396f062dd37494309dbc4238416edd1f8ef293df18b1424813f2fe.cf46c7deb6b9eaa3e47c17b9fc181669c52bc639c165fbc69166a61487662ac9 to temp dir /tmp/tmp7uau9_ep\n",
      "2022-05-08 20:36:45 | INFO | fairseq.tasks.multilingual_masked_lm | dictionary: 250001 types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (encoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(250002, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (12): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (13): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (14): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (15): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (16): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (17): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (18): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (19): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (20): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (21): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (22): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (23): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xlmr = torch.hub.load('pytorch/fairseq', 'xlmr.large').cuda()\n",
    "xlmr.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d1a6b3c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T15:07:27.388228Z",
     "iopub.status.busy": "2022-05-08T15:07:27.387942Z",
     "iopub.status.idle": "2022-05-08T15:07:27.399650Z",
     "shell.execute_reply": "2022-05-08T15:07:27.399018Z",
     "shell.execute_reply.started": "2022-05-08T15:07:27.388204Z"
    }
   },
   "outputs": [],
   "source": [
    "def returnSentenceVectors(sents):\n",
    "    with torch.no_grad():\n",
    "        vectors = []\n",
    "        for line in tqdm(sents):\n",
    "            if isinstance(line, list):\n",
    "                refs = []\n",
    "                for st in line:\n",
    "                    tokens = xlmr.encode(st.strip())\n",
    "                    llf = xlmr.extract_features(tokens)\n",
    "                    llf = llf.mean(dim=1)\n",
    "                    refs.append(llf)\n",
    "                refs = torch.cat(refs, dim=0)\n",
    "                refs = refs.mean(dim=0).unsqueeze(0)\n",
    "                vectors.append(refs)\n",
    "\n",
    "            else:\n",
    "                tokens = xlmr.encode(line)\n",
    "                llf = xlmr.extract_features(tokens)\n",
    "                llf = llf.mean(dim=1)\n",
    "                vectors.append(llf)\n",
    "\n",
    "        vectors = torch.cat(vectors, dim=0)\n",
    "        return vectorsL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aeb38f13",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T15:07:28.877180Z",
     "iopub.status.busy": "2022-05-08T15:07:28.876868Z",
     "iopub.status.idle": "2022-05-08T15:08:22.888560Z",
     "shell.execute_reply": "2022-05-08T15:08:22.887939Z",
     "shell.execute_reply.started": "2022-05-08T15:07:28.877156Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2766/2766 [00:47<00:00, 58.47it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 395/395 [00:06<00:00, 58.99it/s]\n"
     ]
    }
   ],
   "source": [
    "trainSynthVecs = returnSentenceVectors(trainSynth)\n",
    "validSynthVecs = returnSentenceVectors(validSynth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2785e3b2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T15:08:26.597648Z",
     "iopub.status.busy": "2022-05-08T15:08:26.597374Z",
     "iopub.status.idle": "2022-05-08T15:08:26.763001Z",
     "shell.execute_reply": "2022-05-08T15:08:26.762499Z",
     "shell.execute_reply.started": "2022-05-08T15:08:26.597624Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(trainSynthVecs, \"train.synth.pt\")\n",
    "torch.save(validSynthVecs, \"valid.synth.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4a7a759a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T15:08:27.984473Z",
     "iopub.status.busy": "2022-05-08T15:08:27.984242Z",
     "iopub.status.idle": "2022-05-08T15:10:21.644732Z",
     "shell.execute_reply": "2022-05-08T15:10:21.644026Z",
     "shell.execute_reply.started": "2022-05-08T15:08:27.984450Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 2766/2766 [01:53<00:00, 24.34it/s]\n"
     ]
    }
   ],
   "source": [
    "trainHumanVecs = returnSentenceVectors(trainHuman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7c16efb5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T15:10:21.646754Z",
     "iopub.status.busy": "2022-05-08T15:10:21.646529Z",
     "iopub.status.idle": "2022-05-08T15:10:38.085209Z",
     "shell.execute_reply": "2022-05-08T15:10:38.084738Z",
     "shell.execute_reply.started": "2022-05-08T15:10:21.646731Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 395/395 [00:16<00:00, 24.04it/s]\n"
     ]
    }
   ],
   "source": [
    "validHumanVecs = returnSentenceVectors(validHuman)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "13f7c793",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T15:10:38.086258Z",
     "iopub.status.busy": "2022-05-08T15:10:38.086076Z",
     "iopub.status.idle": "2022-05-08T15:10:38.237451Z",
     "shell.execute_reply": "2022-05-08T15:10:38.236798Z",
     "shell.execute_reply.started": "2022-05-08T15:10:38.086236Z"
    }
   },
   "outputs": [],
   "source": [
    "torch.save(trainHumanVecs, \"train.human.pt\")\n",
    "torch.save(validHumanVecs, \"valid.human.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6267046b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cm",
   "language": "python",
   "name": "cm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
