{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4701767-dad0-4201-bba6-70c4bf8e9b87",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T11:40:39.100275Z",
     "iopub.status.busy": "2022-05-09T11:40:39.099869Z",
     "iopub.status.idle": "2022-05-09T11:40:43.337941Z",
     "shell.execute_reply": "2022-05-09T11:40:43.337233Z",
     "shell.execute_reply.started": "2022-05-09T11:40:39.100205Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys, re, pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "import os.path as osp\n",
    "\n",
    "datafolderpath = \"../dataset\"\n",
    "test_csv = \"test - no labels.csv\"\n",
    "test_human_pkl = \"test_human_generated.pkl\"\n",
    "\n",
    "testdf_file_path = \"/home/prashantk/INLG-shared-task/PreCogLTRC-HinglishEval-INLG-2022/dataset/testdf.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6569c280-8c33-4b43-bcb2-4a1964fbf26b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T11:40:52.339105Z",
     "iopub.status.busy": "2022-05-09T11:40:52.338777Z",
     "iopub.status.idle": "2022-05-09T11:40:52.531239Z",
     "shell.execute_reply": "2022-05-09T11:40:52.530691Z",
     "shell.execute_reply.started": "2022-05-09T11:40:52.339081Z"
    }
   },
   "outputs": [],
   "source": [
    "testdf = pd.read_json(testdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fc311679-9f51-46d8-a0d7-9ad7a58651f1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-09T11:40:57.238812Z",
     "iopub.status.busy": "2022-05-09T11:40:57.238569Z",
     "iopub.status.idle": "2022-05-09T11:40:57.248589Z",
     "shell.execute_reply": "2022-05-09T11:40:57.248068Z",
     "shell.execute_reply.started": "2022-05-09T11:40:57.238789Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['index', 'English', 'Hindi', 'Hinglish', 'hum_gen', 'Hinglish_csnliop',\n",
       "       'hum_gen_csnliop', 'hum_gen_norm', 'hum_gen_tokens', 'hum_gen_lid',\n",
       "       'Hinglish_norm', 'Hinglish_lid', 'Hinglish_pos', 'hum_gen_pos',\n",
       "       'Hinglish_cmi_scores', 'Hinglish_burstiness_scores',\n",
       "       'Hinglish_sp_scores', 'symcom_scores_pos', 'symcom_scores_sent',\n",
       "       'NOUN_symcom', 'PART_symcom', 'PRON_symcom', 'ADP_symcom', 'ADJ_symcom',\n",
       "       'ADV_symcom', 'VERB_symcom', 'AUX_symcom', 'CCONJ_symcom',\n",
       "       'SCONJ_symcom', 'DET_symcom', 'PROPN_symcom', 'NUM_symcom',\n",
       "       'INTJ_symcom', 'symcom_+1_count', 'symcom_-1_count', 'symcom_na_count',\n",
       "       'symcom_others', 'Hingilish_xlmr_scores', 'hum_gen_xlmr_scores'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dba1cdbd-3c9e-4112-9909-0b5ed160c3d5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T15:30:21.070396Z",
     "iopub.status.busy": "2022-05-08T15:30:21.070116Z",
     "iopub.status.idle": "2022-05-08T15:30:21.157529Z",
     "shell.execute_reply": "2022-05-08T15:30:21.156964Z",
     "shell.execute_reply.started": "2022-05-08T15:30:21.070372Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In exercise of the powers vested in him by Rule 58 1 paragraph b of the Defence of Hyderabad Rules which correspond to the Defence of India Rules, it is hereby ordered that no person shall unfurl in a ceremonial fashion at any public function the flag of any foreign country nor salute it.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(791, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf = pd.read_csv(osp.join(datafolderpath, test_csv))\n",
    "test_hum_gen_map = pickle.load(open(osp.join(datafolderpath, test_human_pkl), 'rb'))\n",
    "hum_generated_column = []\n",
    "\n",
    "for ind, row in testdf.iterrows():\n",
    "    try:\n",
    "        hum_generated_column.append(test_hum_gen_map[row[\"English\"]])\n",
    "    except:\n",
    "        print(row[\"English\"])\n",
    "        query=\"'\"+row[\"English\"]\n",
    "        hum_generated_column.append(test_hum_gen_map[query])\n",
    "testdf[\"hum_gen\"] = hum_generated_column\n",
    "\n",
    "testdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8239d5c-bc80-4a38-92be-2ae63772a81f",
   "metadata": {},
   "source": [
    "# csnli\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b3740f30-76e5-4ebd-a25a-f74b187fb6fb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T15:42:56.063890Z",
     "iopub.status.busy": "2022-05-08T15:42:56.063653Z",
     "iopub.status.idle": "2022-05-08T15:43:55.163047Z",
     "shell.execute_reply": "2022-05-08T15:43:55.160981Z",
     "shell.execute_reply.started": "2022-05-08T15:42:56.063866Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/prashantk/csnli\n",
      "Loading NMT model parameters.\n",
      "Loading NMT model parameters.\n",
      "The dy.parameter(...) call is now DEPRECATED.\n",
      "        There is no longer need to explicitly add parameters to the computation graph.\n",
      "        Any used parameter will be added automatically.\n",
      "i\ti\ten\n",
      "thght\tthought\ten\n",
      "mosam\tमौसम\thi\n",
      "dfrnt\tdifferent\ten\n",
      "hoga\tहोगा\thi\n",
      "bs\tबस\thi\n",
      "fog\tfog\ten\n",
      "h\tहै\thi\n"
     ]
    }
   ],
   "source": [
    "os.chdir('/home/prashantk/csnli')\n",
    "\n",
    "print(os.getcwd())\n",
    "from three_step_decoding import *\n",
    "\n",
    "\n",
    "tsd = ThreeStepDecoding('/home/prashantk/csnli/lid_models/hinglish', \n",
    "                        htrans='/home/prashantk/csnli/nmt_models/rom2hin.pt', \n",
    "                        etrans='/home/prashantk/csnli/nmt_models/eng2eng.pt')\n",
    "\n",
    "#testing if csnli tool is working\n",
    "print('\\n'.join(['\\t'.join(x) for x in tsd.tag_sent(u'i thght mosam dfrnt hoga bs fog h')]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ac4cff1a-1ada-4076-a7f2-6632aa3a5490",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T15:44:00.067512Z",
     "iopub.status.busy": "2022-05-08T15:44:00.067250Z",
     "iopub.status.idle": "2022-05-08T15:51:03.828865Z",
     "shell.execute_reply": "2022-05-08T15:51:03.828027Z",
     "shell.execute_reply.started": "2022-05-08T15:44:00.067483Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 791/791 [02:48<00:00,  4.68it/s]\n",
      "100%|██████████| 791/791 [04:14<00:00,  3.11it/s]\n"
     ]
    }
   ],
   "source": [
    "def normalise_sent(sent):\n",
    "    try:\n",
    "        ret = list(tsd.tag_sent(sent))\n",
    "    except TypeError:\n",
    "        ret = [sent]\n",
    "        print(f\"TypeError : {sent}\")\n",
    "    return ret\n",
    "              \n",
    "def normalise_df_column(df, column_name):\n",
    "    csnli_op = []\n",
    "    if isinstance(df[column_name][0], str):\n",
    "        csnli_op = [normalise_sent(sent) for sent in tqdm(df[column_name])]\n",
    "        \n",
    "    elif isinstance(df[column_name][0], list):\n",
    "        csnli_op = [[normalise_sent(sent) for sent in sample] for sample in tqdm(df[column_name]) ]\n",
    "        \n",
    "    return csnli_op\n",
    "\n",
    "columns_to_normalise = [\"Hinglish\", \"hum_gen\"]\n",
    "\n",
    "\n",
    "    \n",
    "for col in columns_to_normalise:\n",
    "    testdf[col+\"_csnliop\"] = normalise_df_column(testdf, col)\n",
    "\n",
    "    \n",
    "tokens_all,norm_tokens_all, lid_all = [],[],[]\n",
    "\n",
    "for row in testdf[\"hum_gen_csnliop\"]:\n",
    "    \n",
    "    row_tokens, row_norm_tokens, row_lid = [],[],[]\n",
    "    \n",
    "    for sent in row:\n",
    "        tokens = [token[0] for token in sent]\n",
    "        norm_tokens = [token[1] for token in sent]\n",
    "        lid = [token[2] for token in sent]\n",
    "        \n",
    "        row_tokens.append(tokens)\n",
    "        row_norm_tokens.append(norm_tokens)\n",
    "        row_lid.append(lid)\n",
    "        \n",
    "    tokens_all.append(row_tokens)\n",
    "    norm_tokens_all.append(row_norm_tokens)\n",
    "    lid_all.append(row_lid)\n",
    "\n",
    "testdf[\"hum_gen_norm\"] = norm_tokens_all\n",
    "testdf[\"hum_gen_tokens\"] = tokens_all\n",
    "testdf[\"hum_gen_lid\"] = lid_all   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea111eb9-c56e-4fb9-89a5-36c29d2d5506",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T16:41:34.831600Z",
     "iopub.status.busy": "2022-05-08T16:41:34.831339Z",
     "iopub.status.idle": "2022-05-08T16:41:34.841343Z",
     "shell.execute_reply": "2022-05-08T16:41:34.840860Z",
     "shell.execute_reply.started": "2022-05-08T16:41:34.831575Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['English', 'Hindi', 'Hinglish', 'hum_gen', 'Hinglish_csnliop',\n",
       "       'hum_gen_csnliop', 'hum_gen_norm', 'hum_gen_tokens', 'hum_gen_lid'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testdf.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7dd719-7445-4771-9c25-7832dac8d927",
   "metadata": {},
   "outputs": [],
   "source": [
    "for row in testdf[\"Hinglish_csnliop\"]:\n",
    "    \n",
    "    row_tokens, row_norm_tokens, row_lid = [],[],[]\n",
    "    \n",
    "    for sent in row:\n",
    "        tokens = [token[0] for token in sent]\n",
    "        norm_tokens = [token[1] for token in sent]\n",
    "        lid = [token[2] for token in sent]\n",
    "        \n",
    "        row_tokens.append(tokens)\n",
    "        row_norm_tokens.append(norm_tokens)\n",
    "        row_lid.append(lid)\n",
    "        \n",
    "    tokens_all.append(row_tokens)\n",
    "    norm_tokens_all.append(row_norm_tokens)\n",
    "    lid_all.append(row_lid)\n",
    "\n",
    "testdf[\"hum_gen_norm\"] = norm_tokens_all\n",
    "testdf[\"hum_gen_tokens\"] = tokens_all\n",
    "testdf[\"hum_gen_lid\"] = lid_all   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "429272bd-e49a-4c84-bfe2-ae5c74f8a39f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T16:43:49.399089Z",
     "iopub.status.busy": "2022-05-08T16:43:49.398837Z",
     "iopub.status.idle": "2022-05-08T16:43:49.410229Z",
     "shell.execute_reply": "2022-05-08T16:43:49.409499Z",
     "shell.execute_reply.started": "2022-05-08T16:43:49.399065Z"
    }
   },
   "outputs": [],
   "source": [
    "testdf[\"Hinglish_norm\"] = [[token[1] for token in el] for el in testdf[\"Hinglish_csnliop\"]]\n",
    "testdf[\"Hinglish_lid\"] = [[token[2] for token in el] for el in testdf[\"Hinglish_csnliop\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eb2bd42e-30c9-4500-9617-bbf4b2c202b1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T16:36:36.222795Z",
     "iopub.status.busy": "2022-05-08T16:36:36.222517Z",
     "iopub.status.idle": "2022-05-08T16:36:36.302343Z",
     "shell.execute_reply": "2022-05-08T16:36:36.301601Z",
     "shell.execute_reply.started": "2022-05-08T16:36:36.222750Z"
    }
   },
   "outputs": [],
   "source": [
    "testdf.to_json(testdf_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3df6185-2081-48a2-b0b2-fda2388d3e19",
   "metadata": {},
   "source": [
    "# PoS Tagging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53a2bfb7-8b4b-4539-8a50-2588a3ad53ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T16:39:57.175201Z",
     "iopub.status.busy": "2022-05-08T16:39:57.174950Z",
     "iopub.status.idle": "2022-05-08T16:39:57.231052Z",
     "shell.execute_reply": "2022-05-08T16:39:57.230405Z",
     "shell.execute_reply.started": "2022-05-08T16:39:57.175176Z"
    }
   },
   "outputs": [],
   "source": [
    "testdf = pd.read_json(testdf_file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "791d861d-0a8a-4786-9b8c-b983fe7876e4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T16:44:01.731813Z",
     "iopub.status.busy": "2022-05-08T16:44:01.731569Z",
     "iopub.status.idle": "2022-05-08T16:44:47.350698Z",
     "shell.execute_reply": "2022-05-08T16:44:47.349979Z",
     "shell.execute_reply.started": "2022-05-08T16:44:01.731790Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset universal_dependencies (/home/prashantk/.cache/huggingface/datasets/universal_dependencies/qhe_hiencs/0.0.0/8a3dbde569b161527eabcc186381ac424d4859ff64505fb3da8ac3e0a03c904d)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "035da14a2e7043c8bc270bb72c408f58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 791/791 [00:09<00:00, 81.73it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 791/791 [00:22<00:00, 35.14it/s]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification\n",
    "from datasets import load_dataset, load_metric, concatenate_datasets\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"prakod/en-hi-pos-tagger-symcom\")\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\"prakod/en-hi-pos-tagger-symcom\")\n",
    "\n",
    "\n",
    "def predictposSent(model,sentence):\n",
    "  \n",
    "  tokenized_sentence = tokenizer(sentence,return_tensors='pt')\n",
    "\n",
    "\n",
    "  mask = []\n",
    "  prev_id = None\n",
    "  for ind,id in enumerate(tokenized_sentence.word_ids()):\n",
    "    \n",
    "    if id is None:\n",
    "      mask.append(-100)\n",
    "    elif id == prev_id:\n",
    "      mask.append(-100)\n",
    "    elif id != prev_id:\n",
    "      mask.append(id)\n",
    "    prev_id = id\n",
    "\n",
    "\n",
    "  outputs = model(**tokenized_sentence.to('cuda'))\n",
    "\n",
    "  preds = np.argmax(outputs['logits'].cpu().detach().numpy(), axis=2).squeeze()\n",
    "\n",
    "\n",
    "  true_preds = [\n",
    "      label_list[p] for (p, l) in zip(preds, mask) if l != -100\n",
    "  ]\n",
    "  \n",
    "  \n",
    "  return true_preds\n",
    "\n",
    "\n",
    "sent = ['abbe','keyo', 'fek', 'raha', 'hai', 'andhe', 'news', 'dalal'] \n",
    "        \n",
    "sent_norm = ['अब्बी', 'क्यो', 'फेक', 'रहा', 'है', 'अंधे', 'news', 'डाल' ]\n",
    "\n",
    "model.to('cuda')\n",
    "\n",
    "datasets_UD = load_dataset('/home/prashantk/en-hi-pos-tagger/load_UD_enhics_mod (3).py','qhe_hiencs')\n",
    "\n",
    "label_list = datasets_UD[\"test\"].features[f\"upos\"].feature.names\n",
    "\n",
    "tags_words = predictposSent(model,' '.join(sent))\n",
    "tags_normalised = predictposSent(model,' '.join(sent_norm))\n",
    "\n",
    "testdf[\"Hinglish_pos\"] = [predictposSent(model,' '.join(sent)) for sent in tqdm(testdf[\"Hinglish_norm\"])]\n",
    "testdf[\"hum_gen_pos\"] = [[predictposSent(model,' '.join(sent)) for sent in el] for el in tqdm(testdf[\"hum_gen_norm\"])]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2823620-224e-4187-821b-22f83d6e4243",
   "metadata": {},
   "source": [
    "# code-mix metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3e228b30-1786-4337-ad64-4157fb196c34",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T16:45:03.743438Z",
     "iopub.status.busy": "2022-05-08T16:45:03.743148Z",
     "iopub.status.idle": "2022-05-08T16:45:03.765300Z",
     "shell.execute_reply": "2022-05-08T16:45:03.764831Z",
     "shell.execute_reply.started": "2022-05-08T16:45:03.743414Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "45.45454545454546"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sys.path.append('/home/prashantk/cs_metrics')\n",
    "\n",
    "from cs_metrics import *\n",
    "\n",
    "sample = 'EN EN HI HI UNIV UNIV HI HI EN EN EN HI HI'\n",
    "cmi(sample)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "969615c9-0851-4c1d-b8e7-6e217028ab7b",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-05-08T16:45:05.266865Z",
     "iopub.status.busy": "2022-05-08T16:45:05.266609Z",
     "iopub.status.idle": "2022-05-08T16:45:05.755103Z",
     "shell.execute_reply": "2022-05-08T16:45:05.754549Z",
     "shell.execute_reply.started": "2022-05-08T16:45:05.266841Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "791it [00:00, 12729.27it/s]\n",
      "791it [00:00, 15841.47it/s]\n",
      "791it [00:00, 5908.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 12 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "791it [00:00, 8554.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(791, 32)\n",
      "{'NOUN_symcom': 0.3333333333333333, 'PART_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'ADJ_symcom': -0.6, 'NOUN_symcom': -0.6, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.7777777777777778, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': 1.0, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'NOUN_symcom': 0.5, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'PROPN_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'ADP_symcom': -0.5, 'ADJ_symcom': 0.3333333333333333, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'ADJ_symcom': -0.3333333333333333, 'NOUN_symcom': 1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'NOUN_symcom': 0.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'NUM_symcom': -1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADV_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': 0.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'VERB_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'INTJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'ADJ_symcom': -1.0, 'NOUN_symcom': -0.5, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'NOUN_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'ADJ_symcom': 0.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': 1.0}\n",
      "{'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'ADJ_symcom': 0.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'VERB_symcom': -1.0, 'PROPN_symcom': 1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': 1.0, 'INTJ_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.75, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'PART_symcom': -1.0, 'PRON_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'PROPN_symcom': -1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 0.0, 'NOUN_symcom': 1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': 0.3333333333333333, 'AUX_symcom': -1.0, 'ADJ_symcom': 0.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.6363636363636364, 'ADP_symcom': -0.5, 'ADJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -0.3333333333333333}\n",
      "{'NOUN_symcom': 0.14285714285714285, 'ADP_symcom': -0.6, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.6, 'ADP_symcom': -1.0, 'VERB_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0, 'PROPN_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': 0.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': 1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'CCONJ_symcom': 1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'NOUN_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PROPN_symcom': -1.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADJ_symcom': 0.3333333333333333, 'NOUN_symcom': 1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'ADJ_symcom': -0.5, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'ADJ_symcom': 1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2727272727272727, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'ADV_symcom': -1.0, 'PART_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': 0.5, 'PART_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'VERB_symcom': -0.3333333333333333, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'VERB_symcom': 0.3333333333333333, 'CCONJ_symcom': -1.0, 'DET_symcom': 1.0, 'NOUN_symcom': 1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADV_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'SCONJ_symcom': -0.6666666666666666, 'NOUN_symcom': 0.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 0.0, 'PART_symcom': 1.0, 'CCONJ_symcom': 1.0, 'NUM_symcom': 1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': -0.6, 'ADJ_symcom': 1.0}\n",
      "{'AUX_symcom': -1.0, 'ADJ_symcom': 0.0, 'PRON_symcom': -0.3333333333333333, 'NOUN_symcom': 0.6, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'PART_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.42857142857142855, 'ADP_symcom': -1.0, 'ADJ_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'PROPN_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'ADP_symcom': -0.3333333333333333, 'PRON_symcom': -1.0, 'NUM_symcom': 1.0, 'PART_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.42857142857142855, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'PRON_symcom': 0.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'ADJ_symcom': -0.3333333333333333, 'NOUN_symcom': 0.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PROPN_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.8333333333333334, 'VERB_symcom': -0.7142857142857143, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'ADV_symcom': 0.0, 'DET_symcom': -1.0, 'PROPN_symcom': 1.0, 'PRON_symcom': -1.0}\n",
      "{'ADV_symcom': 1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': 1.0, 'NUM_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.6363636363636364, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': -0.3333333333333333, 'PART_symcom': -1.0, 'ADV_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'NUM_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0, 'INTJ_symcom': -1.0, 'NOUN_symcom': 1.0}\n",
      "{'NOUN_symcom': 1.0, 'DET_symcom': 0.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'PROPN_symcom': 1.0, 'ADJ_symcom': 0.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'NOUN_symcom': -0.3333333333333333, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': 1.0, 'DET_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -0.7142857142857143, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': -1.0, 'PART_symcom': -0.3333333333333333, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADV_symcom': -1.0, 'VERB_symcom': 0.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.25, 'ADJ_symcom': 0.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'NOUN_symcom': 0.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'PROPN_symcom': -1.0, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'NUM_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.4, 'ADP_symcom': -1.0, 'VERB_symcom': -0.5, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'NUM_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'PROPN_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'NOUN_symcom': 1.0}\n",
      "{'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0, 'NOUN_symcom': 0.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADV_symcom': 1.0, 'CCONJ_symcom': -1.0}\n",
      "{'ADV_symcom': 0.0, 'PART_symcom': -1.0, 'VERB_symcom': 0.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -0.5}\n",
      "{'NOUN_symcom': 0.6666666666666666, 'ADP_symcom': -0.3333333333333333, 'DET_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'ADJ_symcom': 1.0, 'NOUN_symcom': 0.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': 0.3333333333333333, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': 1.0, 'SCONJ_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADJ_symcom': 0.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'VERB_symcom': -0.3333333333333333, 'PRON_symcom': 0.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADV_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'NOUN_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PROPN_symcom': 0.0, 'ADV_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': 1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.4, 'NUM_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -0.3333333333333333, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'SCONJ_symcom': -0.3333333333333333, 'PRON_symcom': -0.3333333333333333, 'VERB_symcom': -1.0, 'NOUN_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': -1.0, 'PROPN_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADJ_symcom': 0.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADJ_symcom': 0.6, 'AUX_symcom': 0.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'VERB_symcom': -1.0, 'NOUN_symcom': -0.6666666666666666, 'PART_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PROPN_symcom': -1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'VERB_symcom': 0.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'ADJ_symcom': -0.3333333333333333, 'NOUN_symcom': 1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADV_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.6, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': 1.0, 'NUM_symcom': -1.0}\n",
      "{'VERB_symcom': 0.0, 'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'ADJ_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': 1.0}\n",
      "{'ADJ_symcom': 0.3333333333333333, 'NOUN_symcom': 1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0, 'PROPN_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.7142857142857143, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'NUM_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.6, 'ADJ_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PROPN_symcom': 1.0, 'ADV_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'NOUN_symcom': -0.3333333333333333, 'DET_symcom': -1.0, 'PRON_symcom': 0.0, 'ADV_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'AUX_symcom': 1.0, 'VERB_symcom': 1.0, 'DET_symcom': 1.0, 'NOUN_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'ADV_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADV_symcom': 1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.07692307692307693, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'NUM_symcom': -1.0, 'ADJ_symcom': 0.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'DET_symcom': -1.0, 'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.09090909090909091, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -0.5, 'ADV_symcom': -1.0, 'CCONJ_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'ADJ_symcom': -1.0, 'PROPN_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': 1.0}\n",
      "{'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'NOUN_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'PRON_symcom': 1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'PROPN_symcom': -0.6666666666666666, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'NOUN_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'PRON_symcom': -1.0, 'NUM_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': 0.0, 'NOUN_symcom': 0.0, 'ADJ_symcom': 0.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0, 'PROPN_symcom': 1.0, 'DET_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'NOUN_symcom': 0.5, 'VERB_symcom': -1.0, 'ADV_symcom': 0.0, 'PRON_symcom': -1.0, 'ADJ_symcom': 0.0, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.7142857142857143, 'NUM_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': 1.0, 'DET_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'SCONJ_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'VERB_symcom': -1.0, 'NOUN_symcom': 1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': 0.0, 'NOUN_symcom': 0.0, 'PROPN_symcom': -1.0, 'VERB_symcom': 0.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'PROPN_symcom': 1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'NOUN_symcom': 1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'PROPN_symcom': 1.0, 'NOUN_symcom': 0.3333333333333333, 'VERB_symcom': 1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.42857142857142855, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'PROPN_symcom': 1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'VERB_symcom': -0.6, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 0.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'DET_symcom': 1.0, 'SCONJ_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'NUM_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADP_symcom': -0.7142857142857143, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'PROPN_symcom': 1.0, 'VERB_symcom': 0.0, 'AUX_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'NUM_symcom': -1.0, 'ADV_symcom': -1.0, 'PROPN_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.7142857142857143, 'ADP_symcom': -0.7142857142857143, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'NUM_symcom': -1.0, 'ADJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.0, 'ADV_symcom': -1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.25, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.7777777777777778, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'PROPN_symcom': 1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.7142857142857143, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.25, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -0.42857142857142855, 'AUX_symcom': -1.0, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PROPN_symcom': -1.0, 'ADV_symcom': -0.3333333333333333, 'ADJ_symcom': -1.0, 'SCONJ_symcom': 0.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'PRON_symcom': -0.75, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -0.3333333333333333, 'ADV_symcom': -1.0, 'INTJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6666666666666666, 'CCONJ_symcom': 1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': 1.0}\n",
      "{'VERB_symcom': -0.3333333333333333, 'PART_symcom': -0.3333333333333333, 'NOUN_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'PROPN_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'DET_symcom': -1.0, 'NOUN_symcom': 0.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'NUM_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': -0.3333333333333333, 'PART_symcom': -1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.42857142857142855, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 0.0, 'VERB_symcom': -1.0, 'DET_symcom': 1.0, 'ADV_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'PRON_symcom': -1.0, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'PART_symcom': -1.0, 'ADP_symcom': -1.0, 'SCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'INTJ_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'ADP_symcom': -0.875, 'NOUN_symcom': -0.7142857142857143, 'VERB_symcom': -1.0, 'ADV_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'SCONJ_symcom': -0.3333333333333333, 'CCONJ_symcom': 0.0}\n",
      "{'ADV_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'NOUN_symcom': 1.0, 'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'PART_symcom': -1.0, 'PRON_symcom': -0.3333333333333333, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 0.0, 'DET_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'ADJ_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2857142857142857, 'ADP_symcom': -1.0, 'DET_symcom': -0.6666666666666666, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': 0.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'ADJ_symcom': 0.5, 'PRON_symcom': -1.0, 'VERB_symcom': 0.0, 'CCONJ_symcom': 0.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.7142857142857143, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.38461538461538464, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -0.7142857142857143, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'PROPN_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': 0.0, 'ADV_symcom': 1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'NOUN_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'NUM_symcom': 1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': 0.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.14285714285714285, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PROPN_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': 1.0, 'VERB_symcom': 1.0, 'SCONJ_symcom': -1.0}\n",
      "{'PRON_symcom': 0.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'NOUN_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'CCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'NOUN_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'PROPN_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -0.5, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'ADJ_symcom': 1.0, 'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': 0.0, 'AUX_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADJ_symcom': 1.0, 'PART_symcom': -1.0, 'CCONJ_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.14285714285714285, 'ADP_symcom': -0.6666666666666666, 'VERB_symcom': -1.0, 'ADJ_symcom': 0.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -0.5, 'ADV_symcom': -1.0, 'ADJ_symcom': 1.0, 'PART_symcom': -1.0, 'NUM_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADJ_symcom': 1.0, 'NOUN_symcom': 0.3333333333333333, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6666666666666666, 'ADP_symcom': -1.0, 'SCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'PRON_symcom': -0.6, 'NOUN_symcom': 0.2, 'DET_symcom': -1.0, 'VERB_symcom': 0.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2727272727272727, 'ADP_symcom': -0.7142857142857143, 'ADJ_symcom': 1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'ADJ_symcom': 1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'NUM_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.42857142857142855, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0, 'NUM_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'PROPN_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'NUM_symcom': -1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'NUM_symcom': -1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'AUX_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'AUX_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': 1.0}\n",
      "{'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'NOUN_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.42857142857142855, 'VERB_symcom': -0.2, 'ADV_symcom': -1.0, 'SCONJ_symcom': -0.3333333333333333, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PROPN_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADV_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': 0.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': 1.0, 'PROPN_symcom': -1.0, 'DET_symcom': 1.0}\n",
      "{'ADJ_symcom': 1.0, 'PROPN_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6666666666666666, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'NOUN_symcom': -0.2, 'PRON_symcom': -0.5, 'ADV_symcom': 0.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': 0.0}\n",
      "{'NOUN_symcom': 0.6, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.14285714285714285, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -0.6666666666666666, 'ADJ_symcom': -0.6, 'ADV_symcom': -1.0, 'NUM_symcom': -1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'INTJ_symcom': -1.0, 'PART_symcom': -1.0, 'PROPN_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'PART_symcom': -1.0, 'NOUN_symcom': -1.0, 'SCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'PROPN_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'NOUN_symcom': -1.0, 'ADV_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': 1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'PRON_symcom': -1.0, 'ADJ_symcom': -0.5, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'NOUN_symcom': -1.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADJ_symcom': 0.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'AUX_symcom': -0.3333333333333333, 'VERB_symcom': 0.0, 'ADJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.16666666666666666, 'VERB_symcom': -0.6, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.7142857142857143, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.6842105263157895, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -0.6, 'PART_symcom': -1.0, 'ADV_symcom': -1.0, 'PROPN_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': 0.5, 'PART_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6666666666666666, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'NUM_symcom': -1.0, 'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'CCONJ_symcom': 1.0, 'ADJ_symcom': 1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.0, 'SCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0, 'PROPN_symcom': 1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'NUM_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'AUX_symcom': -0.3333333333333333, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'ADV_symcom': 0.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0, 'SCONJ_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'PART_symcom': -1.0, 'ADV_symcom': -1.0, 'DET_symcom': -1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': 1.0}\n",
      "{'ADP_symcom': -0.6, 'NOUN_symcom': -0.5, 'ADJ_symcom': -0.3333333333333333, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2857142857142857, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PROPN_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0, 'NOUN_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'NOUN_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'PROPN_symcom': 1.0, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0}\n",
      "{'SCONJ_symcom': 0.0, 'NOUN_symcom': 1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': -1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'AUX_symcom': 1.0, 'VERB_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.6666666666666666, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 0.2, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'PROPN_symcom': 1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'DET_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADV_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -0.3333333333333333, 'PART_symcom': -1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'ADV_symcom': 1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.875, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': -1.0, 'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PROPN_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'NOUN_symcom': -1.0, 'ADV_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'DET_symcom': 0.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': -0.14285714285714285, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'PROPN_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.6666666666666666, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'DET_symcom': -0.3333333333333333, 'CCONJ_symcom': -1.0, 'PROPN_symcom': 1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'SCONJ_symcom': -1.0, 'VERB_symcom': -0.3333333333333333, 'NOUN_symcom': 0.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.043478260869565216, 'ADP_symcom': -0.875, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': -0.5, 'DET_symcom': -1.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': 1.0, 'PROPN_symcom': -1.0}\n",
      "{'ADJ_symcom': -1.0, 'NOUN_symcom': 0.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': 1.0, 'NUM_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'VERB_symcom': -0.3333333333333333, 'NOUN_symcom': 0.3333333333333333, 'AUX_symcom': 0.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.5, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'VERB_symcom': 0.0, 'DET_symcom': 0.0, 'SCONJ_symcom': 0.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'SCONJ_symcom': -1.0, 'ADP_symcom': -0.5, 'VERB_symcom': -0.5, 'ADJ_symcom': -1.0, 'PRON_symcom': 0.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'NUM_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADJ_symcom': 0.2, 'AUX_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADV_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'ADJ_symcom': 0.0, 'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'ADJ_symcom': 0.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'AUX_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADV_symcom': -1.0, 'PART_symcom': -1.0, 'ADP_symcom': 1.0}\n",
      "{'PART_symcom': -1.0, 'PRON_symcom': -1.0, 'NOUN_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.14285714285714285, 'ADP_symcom': -0.8, 'ADJ_symcom': 0.5, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'PROPN_symcom': 1.0, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.14285714285714285, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'ADP_symcom': -0.3333333333333333, 'AUX_symcom': 0.0, 'PRON_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.7142857142857143, 'ADP_symcom': -0.6}\n",
      "{'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'NOUN_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3793103448275862, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': 0.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': 0.7142857142857143, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'ADV_symcom': 1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': 0.0, 'NUM_symcom': -1.0, 'ADJ_symcom': -1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'VERB_symcom': 1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.8181818181818182, 'ADP_symcom': -1.0, 'ADJ_symcom': -0.3333333333333333, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'NOUN_symcom': 0.0, 'ADJ_symcom': 1.0, 'ADV_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'DET_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'ADV_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'NOUN_symcom': 1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': 0.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.7142857142857143, 'VERB_symcom': -0.6, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': 1.0, 'ADJ_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'PRON_symcom': 1.0, 'PART_symcom': -1.0, 'NOUN_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': 1.0}\n",
      "{'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'NOUN_symcom': 0.0, 'PRON_symcom': -0.3333333333333333, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'PRON_symcom': -1.0, 'NUM_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'NUM_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PROPN_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.25, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'NUM_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'NUM_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'PRON_symcom': -0.3333333333333333, 'ADJ_symcom': -1.0, 'VERB_symcom': 0.0, 'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PROPN_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.7142857142857143, 'PROPN_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'NUM_symcom': -1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'VERB_symcom': -0.5, 'PRON_symcom': -1.0, 'NOUN_symcom': -0.5, 'PART_symcom': -1.0, 'INTJ_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'ADP_symcom': -0.6666666666666666, 'NOUN_symcom': -0.5, 'VERB_symcom': -1.0, 'ADJ_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'VERB_symcom': -0.3333333333333333, 'NOUN_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'PROPN_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'PROPN_symcom': -1.0, 'ADJ_symcom': -0.5, 'NOUN_symcom': 0.5, 'ADP_symcom': -1.0, 'ADV_symcom': 0.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.625, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': -1.0, 'DET_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'NUM_symcom': 1.0, 'NOUN_symcom': 0.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADP_symcom': -1.0, 'VERB_symcom': 0.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': 1.0}\n",
      "{'PRON_symcom': -0.5, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'AUX_symcom': 1.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0}\n",
      "{'VERB_symcom': 0.0, 'DET_symcom': 0.0, 'NOUN_symcom': 0.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'PRON_symcom': -1.0, 'NOUN_symcom': -0.3333333333333333, 'ADV_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'NUM_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6666666666666666, 'ADP_symcom': -1.0, 'PROPN_symcom': 1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'PROPN_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -0.6, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': 0.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'PROPN_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.2727272727272727, 'ADJ_symcom': -0.6, 'ADP_symcom': -1.0, 'VERB_symcom': -0.2, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADJ_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'DET_symcom': -0.3333333333333333, 'ADP_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'NUM_symcom': -1.0, 'ADJ_symcom': 0.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.8333333333333334, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'VERB_symcom': -0.5, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': -1.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'PROPN_symcom': -1.0, 'SCONJ_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'NOUN_symcom': 1.0, 'NUM_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'PROPN_symcom': 0.0, 'ADJ_symcom': 1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'VERB_symcom': 0.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'NUM_symcom': 1.0}\n",
      "{'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'NOUN_symcom': -1.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'NOUN_symcom': -1.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PROPN_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'NUM_symcom': 0.0, 'ADV_symcom': 0.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'PROPN_symcom': 1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'VERB_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': 0.14285714285714285, 'NUM_symcom': 1.0, 'ADJ_symcom': 0.0, 'VERB_symcom': 0.0, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.45454545454545453, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.6, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'AUX_symcom': -0.6, 'PRON_symcom': -1.0, 'NOUN_symcom': 0.0, 'VERB_symcom': 0.0, 'ADV_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'PROPN_symcom': 1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': 1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.47368421052631576, 'CCONJ_symcom': -0.7142857142857143, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0, 'PART_symcom': -1.0, 'ADP_symcom': -0.6, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'PART_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -0.5, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADJ_symcom': 1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6666666666666666, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'AUX_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6666666666666666, 'ADJ_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': -1.0, 'SCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'PART_symcom': -1.0, 'ADV_symcom': -1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'PROPN_symcom': -1.0, 'NOUN_symcom': 1.0, 'NUM_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.25, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.5, 'VERB_symcom': -1.0, 'DET_symcom': 1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0, 'ADV_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': 1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'VERB_symcom': 0.0, 'AUX_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'CCONJ_symcom': 0.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADP_symcom': -0.3333333333333333, 'PART_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.0, 'SCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'VERB_symcom': 0.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': 0.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.75, 'ADJ_symcom': -0.5, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': 0.0, 'NUM_symcom': -1.0, 'DET_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'PRON_symcom': -1.0, 'NUM_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'DET_symcom': -0.3333333333333333, 'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'CCONJ_symcom': 0.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'PRON_symcom': 0.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'NOUN_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0, 'PART_symcom': -1.0}\n",
      "{'ADJ_symcom': -0.3333333333333333, 'NOUN_symcom': -0.6, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'PROPN_symcom': -1.0, 'ADJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'NOUN_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'VERB_symcom': -0.5, 'CCONJ_symcom': -1.0, 'PRON_symcom': -0.3333333333333333, 'ADJ_symcom': -0.3333333333333333, 'NOUN_symcom': 1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'DET_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.8181818181818182, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': 1.0, 'ADV_symcom': -1.0, 'NOUN_symcom': 1.0, 'CCONJ_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.6666666666666666, 'ADP_symcom': -1.0, 'PROPN_symcom': 1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': -1.0, 'DET_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'NUM_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'VERB_symcom': -0.3333333333333333, 'PRON_symcom': -1.0, 'NOUN_symcom': 0.0, 'SCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'VERB_symcom': 0.2, 'PRON_symcom': -1.0, 'AUX_symcom': -0.3333333333333333, 'SCONJ_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADP_symcom': 1.0, 'DET_symcom': 1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADJ_symcom': -1.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'NUM_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'NUM_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': 1.0, 'ADJ_symcom': 1.0}\n",
      "{'PRON_symcom': -1.0, 'NOUN_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PROPN_symcom': -1.0, 'DET_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'AUX_symcom': -0.3333333333333333, 'VERB_symcom': 0.0, 'ADV_symcom': -1.0, 'DET_symcom': -1.0, 'NOUN_symcom': -1.0, 'CCONJ_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.8, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'VERB_symcom': 1.0, 'ADP_symcom': -1.0, 'PRON_symcom': 0.0, 'DET_symcom': -1.0, 'NOUN_symcom': -1.0, 'ADV_symcom': 1.0}\n",
      "{'NOUN_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'NOUN_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADJ_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.6666666666666666, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': -0.5, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0, 'NUM_symcom': -1.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADJ_symcom': 0.3333333333333333, 'VERB_symcom': -1.0, 'ADV_symcom': -1.0, 'DET_symcom': 0.0, 'ADP_symcom': 0.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0, 'PROPN_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'NUM_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': -1.0, 'PART_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PROPN_symcom': 1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'VERB_symcom': 0.0, 'PROPN_symcom': 1.0, 'AUX_symcom': 1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'VERB_symcom': -1.0, 'NOUN_symcom': 1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.42857142857142855, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'PROPN_symcom': 1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'INTJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'VERB_symcom': 0.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'NUM_symcom': -1.0, 'ADJ_symcom': 1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.7142857142857143, 'PRON_symcom': 1.0, 'AUX_symcom': 1.0, 'ADJ_symcom': 1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': 1.0, 'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'NOUN_symcom': -0.3333333333333333, 'SCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'NOUN_symcom': 0.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.6, 'PRON_symcom': -1.0, 'ADJ_symcom': -0.5, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'PROPN_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.25, 'ADP_symcom': -1.0, 'ADJ_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'NOUN_symcom': 0.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'ADJ_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'NOUN_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.8666666666666667, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.14285714285714285, 'ADJ_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': 0.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADJ_symcom': 0.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.42857142857142855, 'ADP_symcom': -1.0, 'ADJ_symcom': -0.3333333333333333, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'ADP_symcom': -1.0, 'SCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 0.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'PRON_symcom': 0.0, 'PROPN_symcom': 1.0, 'NUM_symcom': -1.0, 'ADJ_symcom': 1.0, 'DET_symcom': -1.0, 'ADV_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADJ_symcom': 1.0, 'PROPN_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': 1.0}\n",
      "{'NUM_symcom': 0.0, 'NOUN_symcom': 1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'PROPN_symcom': -1.0, 'NUM_symcom': -1.0, 'ADJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'ADJ_symcom': 1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADP_symcom': 1.0, 'NOUN_symcom': 1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5833333333333334, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': -0.6666666666666666, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PROPN_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0, 'NUM_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'ADP_symcom': -1.0, 'VERB_symcom': -0.3333333333333333, 'PRON_symcom': -1.0, 'ADJ_symcom': 1.0, 'PROPN_symcom': 1.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADP_symcom': -1.0, 'VERB_symcom': 1.0, 'CCONJ_symcom': 1.0, 'ADJ_symcom': -1.0}\n",
      "{'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'ADV_symcom': 1.0, 'NOUN_symcom': -1.0}\n",
      "{'VERB_symcom': 1.0, 'NOUN_symcom': -1.0, 'ADV_symcom': -1.0, 'PART_symcom': 1.0}\n",
      "{'DET_symcom': -1.0, 'NOUN_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'DET_symcom': 0.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6666666666666666, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'ADJ_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'VERB_symcom': -0.6, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADJ_symcom': 0.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.6, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'NUM_symcom': 0.0, 'AUX_symcom': -1.0, 'PROPN_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'ADV_symcom': 0.0, 'AUX_symcom': -1.0, 'PROPN_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5555555555555556, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'AUX_symcom': -1.0, 'PROPN_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'VERB_symcom': 0.0, 'NOUN_symcom': 0.5, 'ADP_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'PROPN_symcom': 0.3333333333333333, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'PROPN_symcom': 1.0, 'ADP_symcom': 0.0, 'VERB_symcom': -1.0, 'NOUN_symcom': 1.0, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'NOUN_symcom': 0.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'PRON_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5555555555555556, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': 1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'NOUN_symcom': 0.2, 'PRON_symcom': -0.3333333333333333, 'ADJ_symcom': 0.3333333333333333, 'PART_symcom': -1.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6666666666666666, 'CCONJ_symcom': 1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'DET_symcom': -0.3333333333333333, 'ADJ_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADJ_symcom': 1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.25, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'VERB_symcom': 1.0, 'DET_symcom': 1.0}\n",
      "{'PRON_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.4, 'VERB_symcom': -1.0, 'ADJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': 0.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.8333333333333334, 'VERB_symcom': -0.8571428571428571, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PART_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'ADV_symcom': 1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.42857142857142855, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADJ_symcom': 0.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'NUM_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'VERB_symcom': 0.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'NOUN_symcom': 1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.4838709677419355, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.8333333333333334, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'NUM_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': -1.0, 'PROPN_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.14285714285714285, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'ADV_symcom': 0.0, 'NOUN_symcom': 1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -0.3333333333333333, 'ADJ_symcom': 0.0, 'PROPN_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADP_symcom': 0.0, 'PRON_symcom': -1.0, 'PROPN_symcom': -1.0, 'VERB_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.42857142857142855, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 0.6, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0, 'ADV_symcom': -1.0, 'PROPN_symcom': 1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'NUM_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'NUM_symcom': -1.0, 'PROPN_symcom': 0.0, 'ADJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'NUM_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5555555555555556, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'INTJ_symcom': -1.0, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'NUM_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADJ_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.25925925925925924, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'ADV_symcom': 1.0, 'NOUN_symcom': 0.0, 'ADJ_symcom': -1.0, 'DET_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.5, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.1111111111111111, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2857142857142857, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'ADV_symcom': -0.7777777777777778, 'PART_symcom': -1.0, 'ADJ_symcom': -0.42857142857142855, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADJ_symcom': 1.0, 'PROPN_symcom': 1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.25, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': 1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADJ_symcom': 0.0, 'PROPN_symcom': -1.0, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': 1.0, 'PROPN_symcom': 1.0}\n",
      "{'DET_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': 1.0, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'ADV_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': -1.0, 'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.25, 'ADP_symcom': -0.6666666666666666, 'ADJ_symcom': 0.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADJ_symcom': -0.5, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2727272727272727, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -0.5, 'AUX_symcom': -1.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.17647058823529413, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': -0.2, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'PROPN_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'AUX_symcom': -1.0, 'PRON_symcom': 1.0, 'DET_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'VERB_symcom': -1.0, 'NOUN_symcom': -0.5, 'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADV_symcom': 1.0, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'PART_symcom': -1.0, 'PRON_symcom': -1.0, 'NOUN_symcom': 1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'ADV_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'PROPN_symcom': 1.0, 'ADP_symcom': -1.0, 'NOUN_symcom': 0.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.14285714285714285, 'VERB_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADP_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.375, 'AUX_symcom': -0.2727272727272727, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': -0.5, 'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'ADP_symcom': -0.8, 'NOUN_symcom': 0.2, 'ADJ_symcom': -1.0, 'VERB_symcom': -0.5, 'PROPN_symcom': 1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.8888888888888888, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'PROPN_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': -0.3333333333333333, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'NUM_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.17647058823529413, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'PROPN_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'PART_symcom': 1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'NOUN_symcom': 0.3333333333333333, 'VERB_symcom': -1.0, 'ADV_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'VERB_symcom': -0.6, 'NOUN_symcom': 0.0, 'ADJ_symcom': 0.3333333333333333, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'NUM_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'AUX_symcom': 0.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'PRON_symcom': -0.3333333333333333, 'ADP_symcom': -0.3333333333333333, 'VERB_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': 0.0, 'DET_symcom': -1.0, 'ADV_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6666666666666666, 'ADJ_symcom': 0.3333333333333333, 'VERB_symcom': -0.3333333333333333, 'PRON_symcom': 0.0, 'CCONJ_symcom': 1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'NUM_symcom': 1.0, 'SCONJ_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'NOUN_symcom': 1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': 0.0, 'NOUN_symcom': -1.0, 'ADJ_symcom': -1.0, 'PROPN_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'NUM_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.09090909090909091, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PROPN_symcom': -1.0, 'ADJ_symcom': -1.0, 'PART_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'VERB_symcom': 0.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': 0.2, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'NOUN_symcom': -1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'VERB_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.2857142857142857, 'ADP_symcom': -0.8461538461538461, 'PRON_symcom': -0.8181818181818182, 'AUX_symcom': -1.0, 'PROPN_symcom': -0.2, 'VERB_symcom': -1.0, 'CCONJ_symcom': -0.6, 'DET_symcom': -1.0, 'PART_symcom': -0.3333333333333333, 'ADJ_symcom': 0.3333333333333333, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.14285714285714285, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -0.3333333333333333, 'ADJ_symcom': 0.3333333333333333, 'ADV_symcom': -1.0, 'CCONJ_symcom': 0.0, 'ADP_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'AUX_symcom': -1.0, 'ADP_symcom': 0.3333333333333333, 'CCONJ_symcom': 0.0, 'PRON_symcom': -1.0, 'PROPN_symcom': 1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.6, 'PROPN_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'PRON_symcom': 0.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'VERB_symcom': 0.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'VERB_symcom': -0.6, 'AUX_symcom': -1.0, 'NOUN_symcom': -0.3333333333333333, 'ADV_symcom': -1.0, 'DET_symcom': 0.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.6, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': 1.0}\n",
      "{'SCONJ_symcom': -1.0, 'PRON_symcom': 0.0, 'NOUN_symcom': 1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'PROPN_symcom': 1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.6, 'PRON_symcom': 0.0, 'ADV_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': 1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.4, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -0.5, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'PART_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': 0.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.75, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PART_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': 0.0, 'DET_symcom': -1.0, 'VERB_symcom': 0.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'AUX_symcom': -0.6666666666666666, 'VERB_symcom': -0.5, 'ADP_symcom': -1.0, 'DET_symcom': -0.3333333333333333, 'NOUN_symcom': 1.0, 'ADJ_symcom': -1.0, 'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'PROPN_symcom': 1.0}\n",
      "{'ADJ_symcom': -1.0, 'ADV_symcom': -1.0, 'NOUN_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'NOUN_symcom': 1.0, 'CCONJ_symcom': 1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': -1.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'NUM_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'VERB_symcom': 0.5, 'NOUN_symcom': -0.3333333333333333, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': 0.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'PROPN_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADJ_symcom': 0.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'NOUN_symcom': 0.0, 'ADV_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'NUM_symcom': 0.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'ADJ_symcom': 0.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.6666666666666666, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -0.6, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'NOUN_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'ADJ_symcom': 0.0, 'NOUN_symcom': 0.6, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'VERB_symcom': 0.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'AUX_symcom': 1.0, 'NOUN_symcom': -1.0, 'VERB_symcom': -1.0, 'ADP_symcom': 1.0}\n",
      "{'ADP_symcom': -0.8888888888888888, 'NOUN_symcom': -0.42857142857142855, 'VERB_symcom': -1.0, 'ADJ_symcom': -0.5, 'AUX_symcom': -1.0, 'PROPN_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': 0.6, 'PRON_symcom': -1.0, 'ADJ_symcom': 1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'PART_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'VERB_symcom': 1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.2857142857142857, 'ADJ_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'PROPN_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'ADP_symcom': -0.3333333333333333, 'NOUN_symcom': 0.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'NUM_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.0, 'NUM_symcom': -1.0, 'VERB_symcom': 1.0}\n",
      "{'PRON_symcom': -1.0, 'NOUN_symcom': 0.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'ADJ_symcom': 0.3333333333333333, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.42857142857142855, 'ADP_symcom': -1.0, 'VERB_symcom': -0.3333333333333333, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'AUX_symcom': 0.0, 'ADJ_symcom': 1.0, 'NOUN_symcom': 1.0, 'PART_symcom': -1.0}\n",
      "{'AUX_symcom': 0.0, 'VERB_symcom': 1.0, 'ADJ_symcom': 1.0, 'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'SCONJ_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'AUX_symcom': -1.0, 'NUM_symcom': -1.0, 'PRON_symcom': 1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': 0.5, 'VERB_symcom': -1.0, 'PROPN_symcom': 1.0, 'NUM_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'VERB_symcom': 0.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'PART_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.14285714285714285, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.6470588235294118, 'ADJ_symcom': -0.75, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -0.5, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'PROPN_symcom': -1.0, 'ADP_symcom': -1.0, 'NOUN_symcom': 0.0, 'ADV_symcom': -1.0, 'ADJ_symcom': 0.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': 1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'INTJ_symcom': -1.0, 'VERB_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.25, 'ADJ_symcom': 0.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': -0.1111111111111111, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': 0.0, 'DET_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'NOUN_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.0, 'NUM_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'NUM_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.42857142857142855, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'NOUN_symcom': 0.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.15789473684210525, 'ADP_symcom': -1.0, 'PROPN_symcom': -0.75, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'PART_symcom': -1.0, 'VERB_symcom': 0.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': 0.2, 'PROPN_symcom': 0.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'VERB_symcom': 0.0, 'NOUN_symcom': 0.3333333333333333, 'DET_symcom': 0.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADJ_symcom': -0.2, 'ADP_symcom': -1.0, 'PROPN_symcom': 1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -0.3333333333333333, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'PART_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'NUM_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.07692307692307693, 'ADJ_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'NUM_symcom': -1.0, 'DET_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'ADV_symcom': 1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADJ_symcom': 0.6666666666666666, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'VERB_symcom': 0.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': 1.0, 'VERB_symcom': 1.0}\n",
      "{'ADJ_symcom': 0.2, 'NOUN_symcom': 0.2, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'SCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'NOUN_symcom': 0.3333333333333333, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': -1.0, 'CCONJ_symcom': 0.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.6, 'ADP_symcom': -1.0, 'DET_symcom': -0.5, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.42857142857142855, 'ADP_symcom': -0.6666666666666666, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'AUX_symcom': 0.0, 'ADJ_symcom': 1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PROPN_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'PRON_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'NOUN_symcom': -0.3333333333333333, 'PROPN_symcom': 0.0, 'ADP_symcom': -1.0, 'ADV_symcom': 0.0, 'VERB_symcom': 1.0, 'NUM_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': 1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADJ_symcom': 0.3333333333333333, 'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'AUX_symcom': -0.3333333333333333, 'ADJ_symcom': -1.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': 0.3333333333333333, 'ADJ_symcom': 1.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.6923076923076923, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -0.3333333333333333, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.25, 'ADP_symcom': -1.0, 'VERB_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADV_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PROPN_symcom': 1.0}\n",
      "{'AUX_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0, 'PROPN_symcom': 1.0, 'NOUN_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': 0.0, 'NUM_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'NUM_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': 1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'ADJ_symcom': -0.14285714285714285, 'NOUN_symcom': 0.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'ADJ_symcom': -1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': 0.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': -0.6, 'VERB_symcom': -0.5, 'NOUN_symcom': 0.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': 1.0, 'ADJ_symcom': 1.0, 'PRON_symcom': -1.0}\n",
      "{'PROPN_symcom': 1.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'PRON_symcom': 0.0, 'AUX_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'VERB_symcom': -0.6666666666666666, 'PRON_symcom': -0.6, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': 0.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'NUM_symcom': -1.0, 'ADJ_symcom': 1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.5, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'ADJ_symcom': 1.0, 'NOUN_symcom': 1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'NOUN_symcom': -0.2, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0, 'NUM_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'DET_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': 1.0, 'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'AUX_symcom': 0.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NUM_symcom': 1.0, 'ADJ_symcom': 1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'PROPN_symcom': 1.0, 'PRON_symcom': -1.0, 'NUM_symcom': -1.0, 'ADJ_symcom': 1.0, 'NOUN_symcom': 1.0, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.1111111111111111, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADJ_symcom': 1.0, 'PRON_symcom': -1.0, 'ADV_symcom': 1.0, 'PROPN_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.6923076923076923, 'ADP_symcom': -1.0, 'AUX_symcom': -0.6, 'PROPN_symcom': 1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'NUM_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'CCONJ_symcom': -1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.21739130434782608, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'PRON_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'NUM_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'VERB_symcom': 0.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'NOUN_symcom': 0.0, 'PRON_symcom': -1.0, 'ADV_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'NUM_symcom': -1.0, 'VERB_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'VERB_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.2, 'ADP_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'NUM_symcom': -1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.7777777777777778, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PROPN_symcom': 1.0}\n",
      "{'NOUN_symcom': -0.5, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NUM_symcom': 1.0, 'ADP_symcom': 0.0, 'PRON_symcom': -1.0, 'NOUN_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'NUM_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'AUX_symcom': -1.0, 'NOUN_symcom': 1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'DET_symcom': -1.0, 'ADJ_symcom': 0.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.3333333333333333, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'PROPN_symcom': 1.0, 'AUX_symcom': -1.0, 'NOUN_symcom': -1.0, 'PART_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADJ_symcom': 1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'ADJ_symcom': -0.3333333333333333, 'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'PART_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'PRON_symcom': -0.2, 'NOUN_symcom': 0.6, 'AUX_symcom': -1.0, 'ADJ_symcom': 1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'NUM_symcom': 1.0, 'AUX_symcom': 0.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': 1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': 1.0}\n",
      "{'AUX_symcom': -0.3333333333333333, 'NOUN_symcom': 1.0, 'PART_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADV_symcom': -1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.8571428571428571, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0, 'ADV_symcom': -1.0, 'CCONJ_symcom': 1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADV_symcom': -1.0, 'PROPN_symcom': 1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'VERB_symcom': -0.2, 'NOUN_symcom': 0.0, 'CCONJ_symcom': -0.3333333333333333, 'AUX_symcom': 0.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': 1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'PROPN_symcom': 0.0, 'ADJ_symcom': 0.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': 0.6, 'DET_symcom': -1.0, 'ADJ_symcom': 0.0, 'PART_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'VERB_symcom': 0.0, 'NUM_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'NOUN_symcom': 1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADJ_symcom': -1.0, 'PART_symcom': -1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'VERB_symcom': -1.0, 'NOUN_symcom': 1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': 1.0, 'AUX_symcom': -1.0}\n",
      "{'PROPN_symcom': 1.0, 'PRON_symcom': 1.0, 'DET_symcom': 1.0}\n",
      "{'NUM_symcom': -1.0, 'NOUN_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.4, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'NOUN_symcom': -0.6, 'VERB_symcom': -0.6, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADV_symcom': -1.0, 'PROPN_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'ADV_symcom': 1.0, 'ADJ_symcom': 1.0}\n",
      "{'DET_symcom': 0.0, 'NOUN_symcom': 0.0, 'VERB_symcom': 0.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.42857142857142855, 'ADP_symcom': -1.0, 'PROPN_symcom': -0.3333333333333333, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'NUM_symcom': -1.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'CCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.5, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADV_symcom': 0.3333333333333333, 'NUM_symcom': -1.0, 'PART_symcom': -1.0, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'VERB_symcom': 0.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADJ_symcom': 0.0, 'PROPN_symcom': 1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'PROPN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'NOUN_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'PRON_symcom': -0.6666666666666666, 'NOUN_symcom': 0.6, 'VERB_symcom': -0.6, 'ADJ_symcom': 0.0, 'AUX_symcom': 0.0, 'DET_symcom': 1.0, 'PROPN_symcom': -1.0, 'ADP_symcom': -1.0, 'ADV_symcom': 1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.16666666666666666, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': -0.3333333333333333, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADV_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.3333333333333333, 'PRON_symcom': -1.0, 'VERB_symcom': 0.0, 'AUX_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'DET_symcom': -1.0, 'NUM_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'ADJ_symcom': 0.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'PROPN_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'AUX_symcom': -1.0, 'CCONJ_symcom': 0.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'SCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADV_symcom': -1.0, 'ADJ_symcom': 0.0, 'ADP_symcom': 0.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 1.0, 'PRON_symcom': -1.0, 'NUM_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'DET_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'ADJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.2, 'ADP_symcom': -1.0, 'PROPN_symcom': 0.0, 'ADJ_symcom': 0.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'PROPN_symcom': 1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'DET_symcom': -1.0, 'PART_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': -1.0, 'ADJ_symcom': 1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'DET_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'AUX_symcom': 0.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NUM_symcom': -1.0, 'NOUN_symcom': -0.3333333333333333, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'ADV_symcom': -1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'AUX_symcom': -1.0, 'PRON_symcom': -1.0, 'PART_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'PRON_symcom': -1.0, 'DET_symcom': -0.3333333333333333, 'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'CCONJ_symcom': 0.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': 0.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'ADP_symcom': -1.0, 'PART_symcom': -1.0, 'ADJ_symcom': -1.0, 'CCONJ_symcom': -1.0}\n",
      "{'NOUN_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': 0.0, 'CCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'PROPN_symcom': -1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0}\n",
      "{'ADV_symcom': -1.0, 'NOUN_symcom': 1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.75, 'ADP_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': -0.8571428571428571, 'ADP_symcom': -1.0, 'ADJ_symcom': -0.8181818181818182, 'CCONJ_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0, 'PROPN_symcom': 1.0, 'PRON_symcom': -1.0, 'SCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADV_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'VERB_symcom': -1.0, 'NUM_symcom': 1.0, 'SCONJ_symcom': -1.0, 'ADP_symcom': -1.0}\n",
      "{'ADP_symcom': -0.75, 'PROPN_symcom': 1.0, 'NOUN_symcom': 1.0, 'AUX_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PRON_symcom': -1.0, 'VERB_symcom': -1.0, 'ADJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'PROPN_symcom': -1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'AUX_symcom': -1.0, 'NOUN_symcom': -1.0, 'ADJ_symcom': 1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.07692307692307693, 'PRON_symcom': -1.0, 'VERB_symcom': -0.8, 'CCONJ_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0, 'ADV_symcom': -1.0, 'SCONJ_symcom': -1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'NUM_symcom': -1.0, 'ADP_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': -0.2, 'PRON_symcom': -1.0, 'NOUN_symcom': -0.3333333333333333, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'PROPN_symcom': -1.0, 'NUM_symcom': -1.0, 'ADJ_symcom': 1.0, 'PART_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'VERB_symcom': -1.0, 'SCONJ_symcom': -1.0, 'NUM_symcom': -1.0, 'PRON_symcom': -1.0, 'ADJ_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'ADP_symcom': -1.0, 'NOUN_symcom': -0.5, 'PART_symcom': -1.0, 'PROPN_symcom': -1.0, 'VERB_symcom': -1.0, 'AUX_symcom': -1.0}\n",
      "{'NOUN_symcom': 1.0, 'ADP_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': 0.3333333333333333, 'AUX_symcom': -1.0, 'VERB_symcom': -1.0, 'PRON_symcom': -1.0}\n",
      "{'NOUN_symcom': 0.0, 'ADP_symcom': -1.0, 'NUM_symcom': -1.0, 'AUX_symcom': -1.0, 'CCONJ_symcom': -1.0, 'DET_symcom': -1.0, 'ADJ_symcom': -1.0, 'ADV_symcom': -1.0}\n"
     ]
    }
   ],
   "source": [
    "cmi_scores = []\n",
    "\n",
    "for ind, row in tqdm(testdf.iterrows()):\n",
    "    \n",
    "    lid = [l.upper() for l in row[\"Hinglish_lid\"]]\n",
    "    \n",
    "    try:\n",
    "        cmi_score = cmi(' '.join(lid))\n",
    "    \n",
    "    except IndexError as e:\n",
    "        print(lid,e)\n",
    "        cmi_score = 0\n",
    "    \n",
    "    cmi_scores.append(cmi_score)\n",
    "\n",
    "    \n",
    "testdf['Hinglish_cmi_scores'] = cmi_scores\n",
    "\n",
    "def spavg_test(x, k= 2):\n",
    "        \n",
    "    LANG_TAGS = ['EN', 'HI']\n",
    "    OTHER_TAGS = ['UNIV', 'NE','ACRO']\n",
    "\n",
    "    LANG_TAGS = [tag.lower() for tag in LANG_TAGS]\n",
    "    OTHER_TAGS = [tag.lower() for tag in OTHER_TAGS]\n",
    "\n",
    "    if isinstance(x,str):\n",
    "        x = x.split()\n",
    "        \n",
    "    x = [el.lower() for el in x]\n",
    "\n",
    "    count = 0 \n",
    "    mem = None\n",
    "    for l_i, l_j in zip(x,x[1:]):\n",
    "        if l_i in OTHER_TAGS:\n",
    "            continue\n",
    "        if l_i != l_j:\n",
    "            count+=1\n",
    "\n",
    "    return count \n",
    "\n",
    "\n",
    "sp_scores = []\n",
    "\n",
    "for ind, row in tqdm(testdf.iterrows()):\n",
    "    \n",
    "    lid = [l.upper() for l in row[\"Hinglish_lid\"]]\n",
    "        \n",
    "    try:\n",
    "        score = spavg_test(' '.join(lid))\n",
    "    \n",
    "    except IndexError as e:\n",
    "        print(lid,e)\n",
    "        score = np.nan\n",
    "    \n",
    "    sp_scores.append(score)\n",
    "    \n",
    "burstiness_scores = []\n",
    "\n",
    "exceptionCount, IndexErrorCount, bothEnHiPresent = 0,0,0\n",
    "\n",
    "for ind, row in tqdm(testdf.iterrows()):\n",
    "    \n",
    "    lid = [l.upper() for l in row[\"Hinglish_lid\"]]\n",
    "        \n",
    "    try:\n",
    "        score = burstiness(' '.join(lid))\n",
    "    \n",
    "    except IndexError as e:\n",
    "        # print(lid,e)\n",
    "        score = np.nan\n",
    "        IndexErrorCount +=1\n",
    "        \n",
    "    except Exception as e:\n",
    "        # print(lid, e)\n",
    "        score = np.nan\n",
    "        c = Counter(lid)\n",
    "        if 'EN' in c and 'HI' in c:\n",
    "            bothEnHiPresent+=1\n",
    "        exceptionCount +=1\n",
    "    \n",
    "    burstiness_scores.append(score)\n",
    "\n",
    "print(exceptionCount, bothEnHiPresent, IndexErrorCount)\n",
    "\n",
    "testdf['Hinglish_burstiness_scores'] = burstiness_scores\n",
    "testdf['Hinglish_sp_scores'] = sp_scores\n",
    "\n",
    "\n",
    "class CodeMIxSentence():\n",
    "    \n",
    "    def __init__(self, sentence = None,\n",
    "                         tokens = None,\n",
    "                         LID_Tags = None,\n",
    "                         PoS_Tags = None):\n",
    "        \n",
    "        self.sentence = sentence\n",
    "        self.tokens = tokens\n",
    "        self.LID_Tags = LID_Tags\n",
    "        self.PoS_Tags = PoS_Tags\n",
    "        self.length = len(LID_Tags) # has to changed to take length from number of token\n",
    "        \n",
    "        self.LID_count_map = dict(Counter(LID_Tags).most_common())\n",
    "        self.PoS_count_map = dict(Counter(PoS_Tags).most_common())      \n",
    "        \n",
    "        lid_pos_combined = []\n",
    "        \n",
    "        for lid,pos in zip(self.LID_Tags, self.PoS_Tags):\n",
    "            \n",
    "            lid_pos_combined.append(pos+'_'+lid)\n",
    "            \n",
    "        self.LID_POS_count_map = dict(Counter(lid_pos_combined).most_common())\n",
    "\n",
    "\n",
    "class SyMCoM:\n",
    "    \n",
    "    def __init__(self, \n",
    "                LID_tagset = None,\n",
    "                PoS_tagset = None, \n",
    "                L1 = None, \n",
    "                L2 = None):\n",
    "        \n",
    "        self.LID_tagset = LID_tagset\n",
    "        self.PoS_tagset = PoS_tagset\n",
    "        self.l1 = L1\n",
    "        self.l2 = L2\n",
    "        \n",
    "        \n",
    "    def symcom_pos_tags(self, codemixsent_obj):\n",
    "        \n",
    "        symcom_scores_pos_tags = {}\n",
    "        \n",
    "        l1 = self.l1\n",
    "        l2 = self.l2\n",
    "        \n",
    "\n",
    "            \n",
    "                \n",
    "        for postag in codemixsent_obj.PoS_count_map:\n",
    "            \n",
    "            if postag not in ['PUNCT','SYM', 'X']:\n",
    "            \n",
    "                pos_l1 = codemixsent_obj.LID_POS_count_map[postag+'_'+self.l1] if postag+'_'+self.l1 in codemixsent_obj.LID_POS_count_map else 0\n",
    "                pos_l2 = codemixsent_obj.LID_POS_count_map[postag+'_'+self.l2] if postag+'_'+self.l2 in codemixsent_obj.LID_POS_count_map else 0\n",
    "                \n",
    "                try:\n",
    "                    symcom_scores_pos_tags[postag+'_symcom'] = (pos_l1 - pos_l2) / (pos_l1 + pos_l2) \n",
    "                    \n",
    "                except ZeroDivisionError: # to handle cases where there the LID for pos tag is not en or hi. \n",
    "                    pass\n",
    "        return symcom_scores_pos_tags\n",
    "    \n",
    "    \n",
    "        \n",
    "        \n",
    "    def symcom_sentence(self, codemixsent_obj):\n",
    "        \n",
    "        symcom_sentence = 0\n",
    "        \n",
    "        symcom_scores_pos_tags = self.symcom_pos_tags(codemixsent_obj)\n",
    "        \n",
    "        for pos, score in symcom_scores_pos_tags.items():\n",
    "            pos = pos.split('_')[0]\n",
    "            symcom_sentence += abs(score) * (codemixsent_obj.PoS_count_map[pos] / codemixsent_obj.length)     \n",
    "            \n",
    "        return symcom_sentence\n",
    "    \n",
    "symcom = SyMCoM(L1 = 'en',\n",
    "                L2 = 'hi', \n",
    "                LID_tagset = ['hi', 'en', 'ne', 'univ', 'acro'],\n",
    "                PoS_tagset = ['NOUN', 'ADV', 'VERB', 'AUX', 'ADJ', 'ADP', 'PUNCT', 'DET', 'PRON', 'PROPN', 'PART', 'CCONJ', 'SCONJ', 'INTJ', 'NUM', 'SYM','X'])\n",
    "\n",
    "\n",
    "symcom = SyMCoM(L1 = 'en',\n",
    "                L2 = 'hi', \n",
    "                LID_tagset = ['hi', 'en', 'ne', 'univ', 'acro'],\n",
    "                PoS_tagset = ['NOUN', 'ADV', 'VERB', 'AUX', 'ADJ', 'ADP', 'PUNCT', 'DET', 'PRON', 'PROPN', 'PART', 'CCONJ', 'SCONJ', 'INTJ', 'NUM', 'SYM','X'])\n",
    "\n",
    "\n",
    "symcom_scores_pos, symcom_sent = [],[]\n",
    "\n",
    "div_by_zero_error_count, errors  = 0 , []\n",
    "\n",
    "for ind, row in tqdm(testdf.iterrows()):\n",
    "\n",
    "    cm_sentence = CodeMIxSentence(sentence = None,\n",
    "                                     tokens = row['Hinglish_norm'],\n",
    "                                     LID_Tags = row['Hinglish_lid'],\n",
    "                                     PoS_Tags = row['Hinglish_pos'])\n",
    "    symcom_scores_pos.append(symcom.symcom_pos_tags(cm_sentence))\n",
    "    symcom_sent.append(symcom.symcom_sentence(cm_sentence))\n",
    "    \n",
    "    \n",
    "testdf['symcom_scores_pos'] = symcom_scores_pos\n",
    "testdf['symcom_scores_sent'] = symcom_sent\n",
    "\n",
    "df2 = pd.json_normalize(testdf[\"symcom_scores_pos\"])\n",
    "testdf = pd.concat([testdf,df2], axis = 1)\n",
    "print(testdf.shape)\n",
    "testdf.reset_index(inplace = True)\n",
    "\n",
    "testdf.shape\n",
    "\n",
    "\n",
    "def count_plusone(sample):\n",
    "    count = 0\n",
    "    try:\n",
    "        assert isinstance(sample,dict)\n",
    "    except AssertionError:\n",
    "        return np.nan\n",
    "    for k,v in sample.items():\n",
    "        if v == 1:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def count_minus1(sample):\n",
    "    print(sample)\n",
    "    count = 0\n",
    "    for k,v in sample.items():\n",
    "        if v == -1:\n",
    "            count+=1\n",
    "    return count\n",
    "\n",
    "def countNaN(sample):\n",
    "    count = 14 - len(sample.keys()) \n",
    "    return count\n",
    "\n",
    "testdf[\"symcom_+1_count\"] = testdf[\"symcom_scores_pos\"].apply(count_plusone)\n",
    "testdf[\"symcom_-1_count\"] = testdf[\"symcom_scores_pos\"].apply(count_minus1)\n",
    "testdf[\"symcom_na_count\"] = testdf[\"symcom_scores_pos\"].apply(countNaN)\n",
    "testdf[\"symcom_others\"] = testdf.apply(lambda row : 14 - row[\"symcom_+1_count\"] - row[\"symcom_-1_count\"]- row[\"symcom_na_count\"] , axis = 1)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3477db52-5b9a-4327-9cd5-720cff3bb84e",
   "metadata": {},
   "source": [
    "# Minicons scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2d4c5d9-742b-4078-adc7-579d93853d71",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T16:45:53.844951Z",
     "iopub.status.busy": "2022-05-08T16:45:53.844614Z",
     "iopub.status.idle": "2022-05-08T16:53:46.140097Z",
     "shell.execute_reply": "2022-05-08T16:53:46.139423Z",
     "shell.execute_reply.started": "2022-05-08T16:45:53.844925Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c37d347bb05433eb2b9bdc6cfb9ca8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/616 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b58235589a16480e9d43d0a6a0ff39db",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5fbbd00156046c9a5479ef18b606e11",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/8.68M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac7f47a27c67425fb61fd3498c7f1ee2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/2.09G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "791it [01:30,  8.70it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 791/791 [00:00<00:00, 2243201.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of nans in the xlmr results : 56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "791it [03:05,  4.26it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 791/791 [00:00<00:00, 1972469.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of nans in the xlmr results : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from minicons import scorer\n",
    "import torch\n",
    "mlm_model = scorer.MaskedLMScorer('xlm-roberta-large', 'cuda')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tokens_strings = [\" \".join(sample) for sample in testdf[\"Hinglish_norm\"]]\n",
    "\n",
    "\n",
    "results = []\n",
    "for ind, batch in tqdm(enumerate(tokens_strings)):\n",
    "    \n",
    "    # print(len(batch))\n",
    "    try:\n",
    "        score = mlm_model.sequence_score(batch, reduction = lambda x: -x.sum(0).item())\n",
    "\n",
    "        results.extend(score)\n",
    "        \n",
    "    except RuntimeError:\n",
    "               \n",
    "        results.extend([np.nan])\n",
    "        \n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    \n",
    "count= 0\n",
    "\n",
    "for el in tqdm(results):\n",
    "    if el != el:\n",
    "        count+=1\n",
    "        \n",
    "print(f\"Num of nans in the xlmr results : {count}\")\n",
    "\n",
    "testdf[\"Hingilish_xlmr_scores\"] = results\n",
    "\n",
    "\n",
    "tokens_strings = [[\" \".join(sent) for sent in sample] for sample in testdf[\"hum_gen_norm\"]]\n",
    "\n",
    "results = []\n",
    "for ind, batch in tqdm(enumerate(tokens_strings)):\n",
    "\n",
    "    rowscores = []\n",
    "    \n",
    "    for ind1, sent in enumerate(batch):\n",
    "        \n",
    "    \n",
    "        # print(len(batch))\n",
    "        try:\n",
    "            score = mlm_model.sequence_score(sent, reduction = lambda x: -x.sum(0).item())\n",
    "\n",
    "            rowscores.extend(score)\n",
    "\n",
    "        except RuntimeError:\n",
    "\n",
    "            rowscores.extend([np.nan])\n",
    "            \n",
    "    results.append(rowscores)\n",
    "\n",
    "    torch.cuda.empty_cache()\n",
    "        \n",
    "    \n",
    "\n",
    "    \n",
    "count= 0\n",
    "\n",
    "for el in tqdm(results):\n",
    "    \n",
    "    if el != el:\n",
    "        count+=1\n",
    "        \n",
    "print(f\"Num of nans in the xlmr results : {count}\")\n",
    "\n",
    "\n",
    "testdf[\"hum_gen_xlmr_scores\"] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c17f622-6da9-4838-b3c9-301efbfe69c8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T16:54:18.640034Z",
     "iopub.status.busy": "2022-05-08T16:54:18.639789Z",
     "iopub.status.idle": "2022-05-08T16:54:18.650677Z",
     "shell.execute_reply": "2022-05-08T16:54:18.650156Z",
     "shell.execute_reply.started": "2022-05-08T16:54:18.640011Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index : 0\n",
      "English : Late Puranchandra Gupta never allowed any power to force him into restricting his freedom of press.\n",
      "\n",
      "Hindi : स्वर्गीय पूरनचन्द्र गुप्ता कभी भी अपने समाचारपत्र साम्न्त बनने की अपनी इच्छाशक्ति से नहीं डगमगाए।\n",
      "\n",
      "Hinglish : late puranchandra gupta kabhi bhi apne press samnt power ki apni ichchhashakti se nahin dagamgae.\n",
      "\n",
      "hum_gen : ['Late Puranchandra Gupta never allowed apne samacharpatra samant banne ki ichchhasakti se.\\n', 'Swargiy Puranchandra Gupta kabhi bhi apne restricting his freedom of press se nahi dagmagaye.\\n']\n",
      "Hinglish_csnliop : [['late', 'late', 'en'], ['puranchandra', 'puranchandra', 'ne'], ['gupta', 'gupta', 'ne'], ['kabhi', 'कभी', 'hi'], ['bhi', 'भी', 'hi'], ['apne', 'अपने', 'hi'], ['press', 'press', 'en'], ['samnt', 'samnt', 'ne'], ['power', 'power', 'en'], ['ki', 'की', 'hi'], ['apni', 'अपनी', 'hi'], ['ichchhashakti', 'इच्छाशक्ति', 'hi'], ['se', 'से', 'hi'], ['nahin', 'नहीं', 'hi'], ['dagamgae.', 'डगमगाया.', 'hi']]\n",
      "hum_gen_csnliop : [[['Late', 'Late', 'en'], ['Puranchandra', 'Puranchandra', 'ne'], ['Gupta', 'Gupta', 'ne'], ['never', 'never', 'en'], ['allowed', 'allowed', 'en'], ['apne', 'अपने', 'hi'], ['samacharpatra', 'समाचारपत्र', 'hi'], ['samant', 'samant', 'ne'], ['banne', 'बनाने', 'hi'], ['ki', 'की', 'hi'], ['ichchhasakti', 'इच्छाशक्ति', 'hi'], ['se.', 'से.', 'hi']], [['Swargiy', 'Swargiy', 'ne'], ['Puranchandra', 'Puranchandra', 'ne'], ['Gupta', 'Gupta', 'ne'], ['kabhi', 'कभी', 'hi'], ['bhi', 'भी', 'hi'], ['apne', 'अपने', 'hi'], ['restricting', 'restricting', 'en'], ['his', 'his', 'en'], ['freedom', 'freedom', 'en'], ['of', 'of', 'en'], ['press', 'press', 'en'], ['se', 'से', 'hi'], ['nahi', 'नही', 'hi'], ['dagmagaye.', 'डगमगाये.', 'hi']]]\n",
      "hum_gen_norm : [['Late', 'Puranchandra', 'Gupta', 'never', 'allowed', 'अपने', 'समाचारपत्र', 'samant', 'बनाने', 'की', 'इच्छाशक्ति', 'से.'], ['Swargiy', 'Puranchandra', 'Gupta', 'कभी', 'भी', 'अपने', 'restricting', 'his', 'freedom', 'of', 'press', 'से', 'नही', 'डगमगाये.']]\n",
      "hum_gen_tokens : [['Late', 'Puranchandra', 'Gupta', 'never', 'allowed', 'apne', 'samacharpatra', 'samant', 'banne', 'ki', 'ichchhasakti', 'se.'], ['Swargiy', 'Puranchandra', 'Gupta', 'kabhi', 'bhi', 'apne', 'restricting', 'his', 'freedom', 'of', 'press', 'se', 'nahi', 'dagmagaye.']]\n",
      "hum_gen_lid : [['en', 'ne', 'ne', 'en', 'en', 'hi', 'hi', 'ne', 'hi', 'hi', 'hi', 'hi'], ['ne', 'ne', 'ne', 'hi', 'hi', 'hi', 'en', 'en', 'en', 'en', 'en', 'hi', 'hi', 'hi']]\n",
      "Hinglish_norm : ['late', 'puranchandra', 'gupta', 'कभी', 'भी', 'अपने', 'press', 'samnt', 'power', 'की', 'अपनी', 'इच्छाशक्ति', 'से', 'नहीं', 'डगमगाया.']\n",
      "Hinglish_lid : ['en', 'ne', 'ne', 'hi', 'hi', 'hi', 'en', 'ne', 'en', 'hi', 'hi', 'hi', 'hi', 'hi', 'hi']\n",
      "Hinglish_pos : ['ADJ', 'PROPN', 'PROPN', 'ADV', 'PART', 'PRON', 'NOUN', 'NOUN', 'NOUN', 'ADP', 'PRON', 'NOUN', 'ADP', 'PART', 'VERB']\n",
      "hum_gen_pos : [['ADJ', 'PROPN', 'PROPN', 'ADV', 'VERB', 'PRON', 'NOUN', 'NOUN', 'VERB', 'SCONJ', 'NOUN', 'ADP'], ['PROPN', 'PROPN', 'PROPN', 'ADV', 'PART', 'PRON', 'VERB', 'PRON', 'NOUN', 'ADP', 'NOUN', 'ADP', 'PART', 'VERB']]\n",
      "Hinglish_cmi_scores : 25.0\n",
      "Hinglish_burstiness_scores : -0.16273506227329523\n",
      "Hinglish_sp_scores : 4\n",
      "symcom_scores_pos : {'NOUN_symcom': 0.3333333333333333, 'PART_symcom': -1.0, 'PRON_symcom': -1.0, 'ADP_symcom': -1.0, 'ADJ_symcom': 1.0, 'ADV_symcom': -1.0, 'VERB_symcom': -1.0}\n",
      "symcom_scores_sent : 0.6888888888888888\n",
      "NOUN_symcom : 0.3333333333333333\n",
      "PART_symcom : -1.0\n",
      "PRON_symcom : -1.0\n",
      "ADP_symcom : -1.0\n",
      "ADJ_symcom : 1.0\n",
      "ADV_symcom : -1.0\n",
      "VERB_symcom : -1.0\n",
      "AUX_symcom : nan\n",
      "CCONJ_symcom : nan\n",
      "SCONJ_symcom : nan\n",
      "DET_symcom : nan\n",
      "PROPN_symcom : nan\n",
      "NUM_symcom : nan\n",
      "INTJ_symcom : nan\n",
      "symcom_+1_count : 1\n",
      "symcom_-1_count : 5\n",
      "symcom_na_count : 7\n",
      "symcom_others : 1\n",
      "Hingilish_xlmr_scores : 98.5987548828125\n",
      "hum_gen_xlmr_scores : [107.72819137573242, 81.9326171875]\n"
     ]
    }
   ],
   "source": [
    "for ind, row in testdf.iterrows():\n",
    "    for col in testdf.columns:\n",
    "        print(f\"{col} : {row[col]}\")\n",
    "        \n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f47430e6-e87c-4c9e-bf9f-af67fa3255c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T16:57:08.281750Z",
     "iopub.status.busy": "2022-05-08T16:57:08.281446Z",
     "iopub.status.idle": "2022-05-08T16:57:08.371807Z",
     "shell.execute_reply": "2022-05-08T16:57:08.371231Z",
     "shell.execute_reply.started": "2022-05-08T16:57:08.281727Z"
    }
   },
   "outputs": [],
   "source": [
    "testdf.to_json(testdf_file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c490e46-4b6f-42bc-9ebb-188f85d65a64",
   "metadata": {},
   "source": [
    "# xlm-r features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "415cd5e0-8608-4ea7-88c6-c0b4269003e3",
   "metadata": {
    "collapsed": true,
    "execution": {
     "iopub.execute_input": "2022-05-08T17:02:28.659350Z",
     "iopub.status.busy": "2022-05-08T17:02:28.659106Z",
     "iopub.status.idle": "2022-05-08T17:06:13.613161Z",
     "shell.execute_reply": "2022-05-08T17:06:13.612423Z",
     "shell.execute_reply.started": "2022-05-08T17:02:28.659326Z"
    },
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://github.com/pytorch/fairseq/archive/main.zip\" to /home/prashantk/.cache/torch/hub/main.zip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: Not a git repository (or any parent up to mount point /home)\n",
      "Stopping at filesystem boundary (GIT_DISCOVERY_ACROSS_FILESYSTEM not set).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "running build_ext\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "cythoning fairseq/data/data_utils_fast.pyx to fairseq/data/data_utils_fast.cpp\n",
      "cythoning fairseq/data/token_block_utils_fast.pyx to fairseq/data/token_block_utils_fast.cpp\n",
      "building 'fairseq.libbleu' extension\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbleu\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Emitting ninja build file /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/build.ninja...\n",
      "Compiling objects...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[1/2] c++ -MMD -MF /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/module.o.d -pthread -B /home/prashantk/miniconda3/envs/cm/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/prashantk/miniconda3/envs/cm/include/python3.7m -c -c /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/clib/libbleu/module.cpp -o /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/module.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "[2/2] c++ -MMD -MF /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/libbleu.o.d -pthread -B /home/prashantk/miniconda3/envs/cm/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/prashantk/miniconda3/envs/cm/include/python3.7m -c -c /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/clib/libbleu/libbleu.cpp -o /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/libbleu.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=libbleu -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "creating build/lib.linux-x86_64-3.7\n",
      "creating build/lib.linux-x86_64-3.7/fairseq\n",
      "g++ -pthread -shared -B /home/prashantk/miniconda3/envs/cm/compiler_compat -L/home/prashantk/miniconda3/envs/cm/lib -Wl,-rpath=/home/prashantk/miniconda3/envs/cm/lib -Wl,--no-as-needed -Wl,--sysroot=/ /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/libbleu.o /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbleu/module.o -o build/lib.linux-x86_64-3.7/fairseq/libbleu.cpython-37m-x86_64-linux-gnu.so\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "building 'fairseq.data.data_utils_fast' extension\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/data\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Emitting ninja build file /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/build.ninja...\n",
      "Compiling objects...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[1/1] c++ -MMD -MF /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/data/data_utils_fast.o.d -pthread -B /home/prashantk/miniconda3/envs/cm/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include -I/home/prashantk/miniconda3/envs/cm/include/python3.7m -c -c /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/data_utils_fast.cpp -o /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/data/data_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=data_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "In file included from /home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969:0,\n",
      "                 from /home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,\n",
      "                 from /home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4,\n",
      "                 from /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/data_utils_fast.cpp:695:\n",
      "/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n",
      " #warning \"Using deprecated NumPy API, disable it with \" \\\n",
      "  ^\n",
      "creating build/lib.linux-x86_64-3.7/fairseq/data\n",
      "g++ -pthread -shared -B /home/prashantk/miniconda3/envs/cm/compiler_compat -L/home/prashantk/miniconda3/envs/cm/lib -Wl,-rpath=/home/prashantk/miniconda3/envs/cm/lib -Wl,--no-as-needed -Wl,--sysroot=/ /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/data/data_utils_fast.o -o build/lib.linux-x86_64-3.7/fairseq/data/data_utils_fast.cpython-37m-x86_64-linux-gnu.so\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "building 'fairseq.data.token_block_utils_fast' extension\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Emitting ninja build file /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/build.ninja...\n",
      "Compiling objects...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[1/1] c++ -MMD -MF /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.o.d -pthread -B /home/prashantk/miniconda3/envs/cm/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include -I/home/prashantk/miniconda3/envs/cm/include/python3.7m -c -c /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/token_block_utils_fast.cpp -o /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.o -std=c++11 -O3 -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=token_block_utils_fast -D_GLIBCXX_USE_CXX11_ABI=0\n",
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "In file included from /home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include/numpy/ndarraytypes.h:1969:0,\n",
      "                 from /home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include/numpy/ndarrayobject.h:12,\n",
      "                 from /home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include/numpy/arrayobject.h:4,\n",
      "                 from /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/token_block_utils_fast.cpp:696:\n",
      "/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/numpy/core/include/numpy/npy_1_7_deprecated_api.h:17:2: warning: #warning \"Using deprecated NumPy API, disable it with \" \"#define NPY_NO_DEPRECATED_API NPY_1_7_API_VERSION\" [-Wcpp]\n",
      " #warning \"Using deprecated NumPy API, disable it with \" \\\n",
      "  ^\n",
      "/home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/token_block_utils_fast.cpp: In function ‘PyArrayObject* __pyx_f_7fairseq_4data_22token_block_utils_fast__get_slice_indices_fast(PyArrayObject*, PyObject*, int, int, int)’:\n",
      "/home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/token_block_utils_fast.cpp:3382:36: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "       __pyx_t_4 = ((__pyx_v_sz_idx < __pyx_t_10) != 0);\n",
      "                                    ^\n",
      "/home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/data/token_block_utils_fast.cpp:3577:36: warning: comparison between signed and unsigned integer expressions [-Wsign-compare]\n",
      "       __pyx_t_3 = ((__pyx_v_sz_idx < __pyx_t_10) != 0);\n",
      "                                    ^\n",
      "g++ -pthread -shared -B /home/prashantk/miniconda3/envs/cm/compiler_compat -L/home/prashantk/miniconda3/envs/cm/lib -Wl,-rpath=/home/prashantk/miniconda3/envs/cm/lib -Wl,--no-as-needed -Wl,--sysroot=/ /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.o -o build/lib.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.cpython-37m-x86_64-linux-gnu.so\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "building 'fairseq.libbase' extension\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbase\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Emitting ninja build file /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/build.ninja...\n",
      "Compiling objects...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[1/1] c++ -MMD -MF /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbase/balanced_assignment.o.d -pthread -B /home/prashantk/miniconda3/envs/cm/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/TH -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/THC -I/home/prashantk/miniconda3/envs/cm/include/python3.7m -c -c /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/clib/libbase/balanced_assignment.cpp -o /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbase/balanced_assignment.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=libbase -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "g++ -pthread -shared -B /home/prashantk/miniconda3/envs/cm/compiler_compat -L/home/prashantk/miniconda3/envs/cm/lib -Wl,-rpath=/home/prashantk/miniconda3/envs/cm/lib -Wl,--no-as-needed -Wl,--sysroot=/ /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libbase/balanced_assignment.o -L/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/fairseq/libbase.cpython-37m-x86_64-linux-gnu.so\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "building 'fairseq.libnat' extension\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libnat\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Emitting ninja build file /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/build.ninja...\n",
      "Compiling objects...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[1/1] c++ -MMD -MF /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libnat/edit_dist.o.d -pthread -B /home/prashantk/miniconda3/envs/cm/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/TH -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/THC -I/home/prashantk/miniconda3/envs/cm/include/python3.7m -c -c /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/fairseq/clib/libnat/edit_dist.cpp -o /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libnat/edit_dist.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=libnat -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "g++ -pthread -shared -B /home/prashantk/miniconda3/envs/cm/compiler_compat -L/home/prashantk/miniconda3/envs/cm/lib -Wl,-rpath=/home/prashantk/miniconda3/envs/cm/lib -Wl,--no-as-needed -Wl,--sysroot=/ /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/fairseq/clib/libnat/edit_dist.o -L/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/fairseq/libnat.cpython-37m-x86_64-linux-gnu.so\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "building 'alignment_train_cpu_binding' extension\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/examples\n",
      "creating /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/examples/operators\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Emitting ninja build file /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/build.ninja...\n",
      "Compiling objects...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[1/1] c++ -MMD -MF /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/examples/operators/alignment_train_cpu.o.d -pthread -B /home/prashantk/miniconda3/envs/cm/compiler_compat -Wl,--sysroot=/ -Wsign-compare -DNDEBUG -g -fwrapv -O3 -Wall -Wstrict-prototypes -fPIC -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/torch/csrc/api/include -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/TH -I/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/include/THC -I/home/prashantk/miniconda3/envs/cm/include/python3.7m -c -c /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/examples/operators/alignment_train_cpu.cpp -o /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/examples/operators/alignment_train_cpu.o -DTORCH_API_INCLUDE_EXTENSION_H '-DPYBIND11_COMPILER_TYPE=\"_gcc\"' '-DPYBIND11_STDLIB=\"_libstdcpp\"' '-DPYBIND11_BUILD_ABI=\"_cxxabi1011\"' -DTORCH_EXTENSION_NAME=alignment_train_cpu_binding -D_GLIBCXX_USE_CXX11_ABI=0 -std=c++14\n",
      "cc1plus: warning: command line option ‘-Wstrict-prototypes’ is valid for C/ObjC but not for C++\n",
      "g++ -pthread -shared -B /home/prashantk/miniconda3/envs/cm/compiler_compat -L/home/prashantk/miniconda3/envs/cm/lib -Wl,-rpath=/home/prashantk/miniconda3/envs/cm/lib -Wl,--no-as-needed -Wl,--sysroot=/ /home/prashantk/.cache/torch/hub/pytorch_fairseq_main/build/temp.linux-x86_64-3.7/examples/operators/alignment_train_cpu.o -L/home/prashantk/miniconda3/envs/cm/lib/python3.7/site-packages/torch/lib -lc10 -ltorch -ltorch_cpu -ltorch_python -o build/lib.linux-x86_64-3.7/alignment_train_cpu_binding.cpython-37m-x86_64-linux-gnu.so\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "copying build/lib.linux-x86_64-3.7/fairseq/libbleu.cpython-37m-x86_64-linux-gnu.so -> fairseq\n",
      "copying build/lib.linux-x86_64-3.7/fairseq/data/data_utils_fast.cpython-37m-x86_64-linux-gnu.so -> fairseq/data\n",
      "copying build/lib.linux-x86_64-3.7/fairseq/data/token_block_utils_fast.cpython-37m-x86_64-linux-gnu.so -> fairseq/data\n",
      "copying build/lib.linux-x86_64-3.7/fairseq/libbase.cpython-37m-x86_64-linux-gnu.so -> fairseq\n",
      "copying build/lib.linux-x86_64-3.7/fairseq/libnat.cpython-37m-x86_64-linux-gnu.so -> fairseq\n",
      "copying build/lib.linux-x86_64-3.7/alignment_train_cpu_binding.cpython-37m-x86_64-linux-gnu.so -> \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-05-08 22:34:03 | INFO | fairseq.file_utils | http://dl.fbaipublicfiles.com/fairseq/models/xlmr.large.tar.gz not found in cache, downloading to /tmp/tmpvolkkwsj\n",
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 1028340964/1028340964 [01:04<00:00, 15857432.14B/s]\n",
      "2022-05-08 22:35:08 | INFO | fairseq.file_utils | copying /tmp/tmpvolkkwsj to cache at /home/prashantk/.cache/torch/pytorch_fairseq/3f864e15bb396f062dd37494309dbc4238416edd1f8ef293df18b1424813f2fe.cf46c7deb6b9eaa3e47c17b9fc181669c52bc639c165fbc69166a61487662ac9\n",
      "2022-05-08 22:35:18 | INFO | fairseq.file_utils | creating metadata file for /home/prashantk/.cache/torch/pytorch_fairseq/3f864e15bb396f062dd37494309dbc4238416edd1f8ef293df18b1424813f2fe.cf46c7deb6b9eaa3e47c17b9fc181669c52bc639c165fbc69166a61487662ac9\n",
      "2022-05-08 22:35:18 | INFO | fairseq.file_utils | removing temp file /tmp/tmpvolkkwsj\n",
      "2022-05-08 22:35:18 | INFO | fairseq.file_utils | loading archive file http://dl.fbaipublicfiles.com/fairseq/models/xlmr.large.tar.gz from cache at /home/prashantk/.cache/torch/pytorch_fairseq/3f864e15bb396f062dd37494309dbc4238416edd1f8ef293df18b1424813f2fe.cf46c7deb6b9eaa3e47c17b9fc181669c52bc639c165fbc69166a61487662ac9\n",
      "2022-05-08 22:35:18 | INFO | fairseq.file_utils | extracting archive file /home/prashantk/.cache/torch/pytorch_fairseq/3f864e15bb396f062dd37494309dbc4238416edd1f8ef293df18b1424813f2fe.cf46c7deb6b9eaa3e47c17b9fc181669c52bc639c165fbc69166a61487662ac9 to temp dir /tmp/tmpxwmpg91r\n",
      "2022-05-08 22:35:59 | INFO | fairseq.tasks.multilingual_masked_lm | dictionary: 250001 types\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaHubInterface(\n",
       "  (model): RobertaModel(\n",
       "    (encoder): RobertaEncoder(\n",
       "      (sentence_encoder): TransformerEncoder(\n",
       "        (dropout_module): FairseqDropout()\n",
       "        (embed_tokens): Embedding(250002, 1024, padding_idx=1)\n",
       "        (embed_positions): LearnedPositionalEmbedding(514, 1024, padding_idx=1)\n",
       "        (layernorm_embedding): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "        (layers): ModuleList(\n",
       "          (0): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (1): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (2): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (3): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (4): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (5): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (6): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (7): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (8): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (9): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (10): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (11): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (12): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (13): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (14): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (15): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (16): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (17): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (18): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (19): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (20): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (21): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (22): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "          (23): TransformerEncoderLayerBase(\n",
       "            (self_attn): MultiheadAttention(\n",
       "              (dropout_module): FairseqDropout()\n",
       "              (k_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (v_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (q_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (out_proj): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "            )\n",
       "            (self_attn_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout_module): FairseqDropout()\n",
       "            (activation_dropout_module): FairseqDropout()\n",
       "            (fc1): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (fc2): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (final_layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (lm_head): RobertaLMHead(\n",
       "        (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "        (layer_norm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      )\n",
       "    )\n",
       "    (classification_heads): ModuleDict()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from tqdm import tqdm \n",
    "\n",
    "xlmr = torch.hub.load('pytorch/fairseq', 'xlmr.large').cuda()\n",
    "xlmr.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4b41cc9-6267-47e0-bc30-6a72742843d2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-05-08T17:06:59.698705Z",
     "iopub.status.busy": "2022-05-08T17:06:59.698500Z",
     "iopub.status.idle": "2022-05-08T17:07:46.180331Z",
     "shell.execute_reply": "2022-05-08T17:07:46.179658Z",
     "shell.execute_reply.started": "2022-05-08T17:06:59.698681Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "791 791\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 791/791 [00:13<00:00, 58.91it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 791/791 [00:32<00:00, 24.39it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "testdf = pd.read_json(testdf_file_path)\n",
    "\n",
    "def returnSynthSentences(df):\n",
    "    csnliHinglish = list(df[\"Hinglish_csnliop\"])\n",
    "    sentences = []\n",
    "    for line in csnliHinglish:\n",
    "        sent = []\n",
    "        for token in line:\n",
    "            sent.append(token[1])\n",
    "        sent = \" \".join(sent)\n",
    "        sentences.append(sent)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "def returnHumSentences(df):\n",
    "    csnliHinglish = list(df[\"hum_gen_csnliop\"])\n",
    "    sentences = []\n",
    "    for sents in csnliHinglish:\n",
    "        ref = []\n",
    "        for line in sents:\n",
    "            sent = []\n",
    "            for token in line:\n",
    "                sent.append(token[1])\n",
    "            sent = \" \".join(sent)\n",
    "            ref.append(sent)\n",
    "            \n",
    "        sentences.append(ref)\n",
    "    \n",
    "    return sentences\n",
    "\n",
    "testSynth = returnSynthSentences(testdf)\n",
    "\n",
    "testHuman = returnHumSentences(testdf)\n",
    "\n",
    "\n",
    "print(len(testSynth), len(testHuman))\n",
    "\n",
    "\n",
    "\n",
    "def returnSentenceVectors(sents):\n",
    "    with torch.no_grad():\n",
    "        vectors = []\n",
    "        for line in tqdm(sents):\n",
    "            if isinstance(line, list):\n",
    "                refs = []\n",
    "                for st in line:\n",
    "                    tokens = xlmr.encode(st.strip())\n",
    "                    llf = xlmr.extract_features(tokens)\n",
    "                    llf = llf.mean(dim=1)\n",
    "                    refs.append(llf)\n",
    "                refs = torch.cat(refs, dim=0)\n",
    "                refs = refs.mean(dim=0).unsqueeze(0)\n",
    "                vectors.append(refs)\n",
    "\n",
    "            else:\n",
    "                tokens = xlmr.encode(line)\n",
    "                llf = xlmr.extract_features(tokens)\n",
    "                llf = llf.mean(dim=1)\n",
    "                vectors.append(llf)\n",
    "\n",
    "        vectors = torch.cat(vectors, dim=0)\n",
    "        return vectors\n",
    "    \n",
    "    \n",
    "testSynthVecs = returnSentenceVectors(testSynth)\n",
    "testHumanVecs = returnSentenceVectors(testHuman)\n",
    "\n",
    "torch.save(testSynthVecs, \"test.synth.pt\")\n",
    "torch.save(testHumanVecs, \"test.human.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3999d962-e262-45f1-9416-b456d6376dbf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cm",
   "language": "python",
   "name": "cm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
